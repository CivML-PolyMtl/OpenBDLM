<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of SGD</title>
  <meta name="keywords" content="SGD">
  <meta name="description" content="INPUTS:">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="../../index.html">BDLM_DATA_LOADER_9</a> &gt; <a href="#">functions</a> &gt; <a href="index.html">ModelParametersLearning</a> &gt; SGD.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for BDLM_DATA_LOADER_9/functions/ModelParametersLearning&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>SGD
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>INPUTS:</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [optim] = SGD(data, model, misc, varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> INPUTS:
 maxIter                   - Maximal number of epochs. Defaut is 30.
 Ndata4miniBatch           - Size of mini batch. Defaut is 0.2 of the training
                             data.
 alpha_split               - Portion of the validation data. Defaut is 0.3.
 optim_mode                - Optimization method can be either MLE or MAP. 
                             Defaut is MLE.
 optimizer                 - optimizer can be Adaptive Moment Estimation (ADAM) 
                             or Momentum (MMT). There are also 2
                             alternatives; ADAMbeta, MMTbeta. Defaut is MMT.
                             ADAM    : See Kingma and Lei Ba (2017)
                             ADAMbeta: See Schaul et al. (2013)
 metric_mode               - metric can be either prediction capacity (predCap)
                             or log-likelihood (logpdf). Defaut is predCap.
 learningRateDefaut        - Manual tuning learning rate
 learningRate_mode         - Learning rate can be hessian, decay, or defaut.
                             hessian -&gt; learningRate = 1/abs(hessian)
                             decay   -&gt; learningRate = learningRateDefaut/sqrt(t)
                             defaut  -&gt; learningRate = learningRateDefaut.
                             Defaut is hessian.
 beta_1                    - Average coefficient for the 1st moment (gradient).
                             Defaut is 0.9.
 beta_2                    - Average coefficient for the 2nd moment
                             (gradient^2). Defaut is 0.999.
 epsilon                   - Coefficient in ADAM. Defaut is 1E-8.
 termination_tolerance     - Moving control coefficient for metric. Defaut
                             is 0.95.</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="../../../BDLM_DATA_LOADER_9/functions/ModelConfiguration/day2sampleIndex.html" class="code" title="function [Index]=day2sampleIndex(day, timestamps)">day2sampleIndex</a>	DAY2SAMPLEINDEX converts a number of days to timestamp index</li><li><a href="logPosteriorPE.html" class="code" title="function [logpdf, Glogpdf, Hlogpdf, delta_grad] = logPosteriorPE(data, model, option, varargin)">logPosteriorPE</a>	INPUTS:</li><li><a href="metricFct.html" class="code" title="function [metricVL, idxMaxM, logpdf_test, logpdf_train] = metricFct(data_train, data_test, model, option, parameterSearch, parameterSearchTR)">metricFct</a>	</li><li><a href="parameter_transformation_fct.html" class="code" title="function [fct_TR,fct_inv_TR,grad_TR2OR,hessian_TR2OR]=parameter_transformation_fct(model,param_idx_loop)">parameter_transformation_fct</a>	</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="learnModelParameters.html" class="code" title="function [data, model, estimation, misc]=learnModelParameters(data, model, estimation, misc, varargin)">learnModelParameters</a>	LEARNMODELPARAMETERS Learn Bayesian dynamic linear model parameters</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function [data_train, data_valid] = dataSplit(data, idxTrain, alpha_split, varargin)</a></li><li><a href="#_sub2" class="code">function [xsearch, xsearchTR, momentumTR] = MMT(xsearchTRprev, momentumTRprev, grad, step, beta, fctInvTR)</a></li><li><a href="#_sub3" class="code">function [xsearch, xsearchTR, momentumTR, mmtHessianTR] = MMTbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, grad, hess, beta, fctInvTR)</a></li><li><a href="#_sub4" class="code">function [xsearch, xsearchTR, momentumTR, RMSpropTR] = ADAM(xsearchTRprev, momentumTRprev, RMSpropTRprev, grad, step, beta_1, beta_2, epsilon, Niter, fctInvTR)</a></li><li><a href="#_sub5" class="code">function [xsearch, xsearchTR, momentumTR, RMSpropTR, mmtHessianTR] = ADAMbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, RMSpropTRprev, grad, hess, beta_1, beta_2, epsilon, Niter, fctInvTR)</a></li><li><a href="#_sub6" class="code">function [xM, momentumM, RMSpropM, gradM, learningRateM, mmtHessM, hessM]= paramGrid(x, momentum, RMSprop, grad, learningRate, mmtHess, hess)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [optim] = SGD(data, model, misc, varargin)</a>
0002 <span class="comment">% INPUTS:</span>
0003 <span class="comment">% maxIter                   - Maximal number of epochs. Defaut is 30.</span>
0004 <span class="comment">% Ndata4miniBatch           - Size of mini batch. Defaut is 0.2 of the training</span>
0005 <span class="comment">%                             data.</span>
0006 <span class="comment">% alpha_split               - Portion of the validation data. Defaut is 0.3.</span>
0007 <span class="comment">% optim_mode                - Optimization method can be either MLE or MAP.</span>
0008 <span class="comment">%                             Defaut is MLE.</span>
0009 <span class="comment">% optimizer                 - optimizer can be Adaptive Moment Estimation (ADAM)</span>
0010 <span class="comment">%                             or Momentum (MMT). There are also 2</span>
0011 <span class="comment">%                             alternatives; ADAMbeta, MMTbeta. Defaut is MMT.</span>
0012 <span class="comment">%                             ADAM    : See Kingma and Lei Ba (2017)</span>
0013 <span class="comment">%                             ADAMbeta: See Schaul et al. (2013)</span>
0014 <span class="comment">% metric_mode               - metric can be either prediction capacity (predCap)</span>
0015 <span class="comment">%                             or log-likelihood (logpdf). Defaut is predCap.</span>
0016 <span class="comment">% learningRateDefaut        - Manual tuning learning rate</span>
0017 <span class="comment">% learningRate_mode         - Learning rate can be hessian, decay, or defaut.</span>
0018 <span class="comment">%                             hessian -&gt; learningRate = 1/abs(hessian)</span>
0019 <span class="comment">%                             decay   -&gt; learningRate = learningRateDefaut/sqrt(t)</span>
0020 <span class="comment">%                             defaut  -&gt; learningRate = learningRateDefaut.</span>
0021 <span class="comment">%                             Defaut is hessian.</span>
0022 <span class="comment">% beta_1                    - Average coefficient for the 1st moment (gradient).</span>
0023 <span class="comment">%                             Defaut is 0.9.</span>
0024 <span class="comment">% beta_2                    - Average coefficient for the 2nd moment</span>
0025 <span class="comment">%                             (gradient^2). Defaut is 0.999.</span>
0026 <span class="comment">% epsilon                   - Coefficient in ADAM. Defaut is 1E-8.</span>
0027 <span class="comment">% termination_tolerance     - Moving control coefficient for metric. Defaut</span>
0028 <span class="comment">%                             is 0.95.</span>
0029 
0030 <span class="comment">% OUTPUTS:</span>
0031 <span class="comment">% optim.parameter_opt       - optimal parameters in original space.</span>
0032 <span class="comment">% optim.parameterTR_opt     - optimal parameters in transformed space.</span>
0033 
0034 <span class="comment">% TIPS:</span>
0035 <span class="comment">% - THE HEAVILY COMPUTATIONNAL RESOURCE IS REQUIRED FOR THIS ALGORITHM</span>
0036 <span class="comment">% - If optimizer is ADAM, learningRate_mode should be either decay or</span>
0037 <span class="comment">%   defaut. Also, ADAM work well with a small training set.</span>
0038 <span class="comment">% - If optimizer is MMT, learningRate_mode should be hessian.</span>
0039 <span class="comment">%   MMT converge fast to the maximum point, but it is require a decent</span>
0040 <span class="comment">%   amount of data such &gt; 2000 (data point).</span>
0041 <span class="comment">% - ADAM     -&gt; learningRate_mode = defaut</span>
0042 <span class="comment">% - ADAMbeta -&gt; learningRate_mode = hessian</span>
0043 <span class="comment">% - MMT      -&gt; learningRate_mode = hessian</span>
0044 <span class="comment">% - MMTbeta  -&gt; learningRate_mode = hessian</span>
0045 <span class="comment">% - predCap is recommended as the metric for optimization process</span>
0046 <span class="comment">% - The hyperparameter such as alpha_split, learningRateDefaut, stdInit,</span>
0047 <span class="comment">%   beta_1, beta_2, epsilon, termination_tolorance are recommended to set</span>
0048 <span class="comment">%   as defaut for an effective performance.</span>
0049 <span class="comment">%% Defaut values</span>
0050 warning(<span class="string">'SGD ALGORITHM IS RECOMMENDED TO RUN ON MULTI-PROCESSORS'</span>)
0051 maxIter                 = 5;
0052 alpha_split             = 0.3;
0053 misc.optim_mode       = <span class="string">'MLE'</span>;
0054 misc.optimizer        = <span class="string">'MMT'</span>;
0055 misc.metric_mode      = <span class="string">'predCap'</span>;
0056 learningRate_mode       = <span class="string">'hessian'</span>;
0057 disp_flag               = 1;
0058 learningRateDefaut      = 5E-3;
0059 stdInit                 = 0.5;
0060 beta_1                  = 0.9;
0061 beta_2                  = 0.999;
0062 epsilon                 = 1E-8;
0063 termination_tolerance   = 0.95;
0064 timeLimit               = misc.time_limit_calibration*60;
0065 hessianDefaut           = 1000;
0066 misc.parallel         = 0;
0067 <span class="comment">%% Data</span>
0068 
0069 <span class="comment">% Get training period</span>
0070 training_start_idx = <a href="../../../BDLM_DATA_LOADER_9/functions/ModelConfiguration/day2sampleIndex.html" class="code" title="function [Index]=day2sampleIndex(day, timestamps)">day2sampleIndex</a>(misc.trainingPeriod(1), data.timestamps);
0071 training_end_idx = <a href="../../../BDLM_DATA_LOADER_9/functions/ModelConfiguration/day2sampleIndex.html" class="code" title="function [Index]=day2sampleIndex(day, timestamps)">day2sampleIndex</a>(misc.trainingPeriod(2), data.timestamps);
0072 
0073 misc.training_start_idx = training_start_idx;
0074 misc.training_end_idx =  training_end_idx;
0075 
0076 idxTrain                    = misc.training_start_idx:misc.training_end_idx;
0077 [data_train, data_valid]    = <a href="#_sub1" class="code" title="subfunction [data_train, data_valid] = dataSplit(data, idxTrain, alpha_split, varargin)">dataSplit</a>(data, idxTrain, alpha_split);
0078 data.dt_ref                 = data_train.dt_ref;
0079 data.dt_steps(1)            = data.dt_ref;
0080 Ndata4miniBatch             = round(0.2*size(data_valid.values,1)); 
0081 NminiBatch                  = ceil(length(data_train.values)/Ndata4miniBatch);
0082 <span class="keyword">if</span> NminiBatch&lt;2
0083    NminiBatch=1; 
0084 <span class="keyword">end</span>
0085 <span class="comment">%% If provided, employ user-specific arguments</span>
0086 args    = varargin;
0087 nargs   = length(varargin);
0088 <span class="keyword">for</span> n = 1:2:nargs
0089     <span class="keyword">switch</span> args{n}
0090         <span class="keyword">case</span> <span class="string">'maxEpoch'</span>,            maxIter             = args{n+1};
0091         <span class="keyword">case</span> <span class="string">'Ndata4miniBatch'</span>,     Ndata4miniBatch     = args{n+1};
0092         <span class="keyword">case</span> <span class="string">'validationSet'</span>,       alpha_split         = args{n+1};
0093         <span class="keyword">case</span> <span class="string">'optim_mode'</span>,          misc.optim_mode   = args{n+1};
0094         <span class="keyword">case</span> <span class="string">'optimizer'</span>,           misc.optimizer    = args{n+1}; 
0095         <span class="keyword">case</span> <span class="string">'metric_mode'</span>,         misc.metric_mode  = args{n+1};
0096         <span class="keyword">case</span> <span class="string">'learningRate_mode'</span>,   learningRate_mode   = args{n+1};
0097         <span class="keyword">case</span> <span class="string">'learningRate'</span>,        learningRateDefaut  = args{n+1};
0098         <span class="keyword">case</span> <span class="string">'beta_1'</span>,              beta_1              = args{n+1};
0099         <span class="keyword">case</span> <span class="string">'beta_2'</span>,              beta_2              = args{n+1};
0100         <span class="keyword">case</span> <span class="string">'epsilon'</span>,             epsilon             = args{n+1};
0101         <span class="keyword">case</span> <span class="string">'termination_tolerance'</span>,termination_tolerance = args{n+1};
0102         <span class="keyword">otherwise</span>, error([<span class="string">'unrecognized argument'</span> args{n}])
0103     <span class="keyword">end</span>
0104 <span class="keyword">end</span>
0105 
0106 <span class="keyword">if</span> disp_flag==1
0107     <span class="comment">%% Diaplay analysis parameters</span>
0108     disp(<span class="string">'----------------------------------------------------------------------------------------------'</span>)
0109     disp(<span class="string">'Stochastic Gradient Descent for the Bayesian Dynamic Linear Models'</span>)
0110     disp(<span class="string">'----------------------------------------------------------------------------------------------'</span>)
0111     disp(<span class="string">' '</span>)
0112     disp(<span class="string">'    \\start SGD algorithm (finite difference method)'</span>)
0113     disp(<span class="string">' '</span>)
0114     disp([<span class="string">'      Optimization mode                                      '</span> misc.optim_mode])
0115     disp([<span class="string">'      Optimizer                                              '</span> misc.optimizer])
0116     disp([<span class="string">'      Metric                                                 '</span> misc.metric_mode])
0117     disp([<span class="string">'      Learning Rate mode                                     '</span> learningRate_mode])
0118     disp([<span class="string">'      Training period:                                       '</span> num2str(misc.trainingPeriod(1)) <span class="string">'-'</span> num2str(misc.trainingPeriod(2)) <span class="string">' [days]'</span>])
0119     disp([<span class="string">'      Validation set portion:                                '</span> num2str(alpha_split) <span class="string">' [%]'</span>])
0120     disp([<span class="string">'      Trainig set:                                           '</span> num2str(size(data_train.values,1)) <span class="string">' [data points]'</span>])
0121     disp([<span class="string">'      Validation set:                                        '</span> num2str(size(data_valid.values,1)-size(data_train.values,1)) <span class="string">' [data points]'</span>])    
0122     disp([<span class="string">'      Mini batch:                                            '</span> num2str(Ndata4miniBatch) <span class="string">' [data points]'</span>])
0123     disp([<span class="string">'      Number of max epoch:                                   '</span> num2str(maxIter) <span class="string">' [epochs]'</span>])
0124     disp([<span class="string">'      Total time limit for calibration:                      '</span> num2str(misc.time_limit_calibration) <span class="string">' [min]'</span>])
0125     disp(<span class="string">' '</span>)
0126     disp(<span class="string">'    ...in progress'</span>)
0127     disp(<span class="string">' '</span>)
0128 <span class="keyword">end</span>
0129 <span class="comment">%% Initialize transformed model parameters</span>
0130 <span class="comment">%Identify parameters to be optimized</span>
0131 parameter_search_idx    = find(~all(isnan(reshape([model.param_properties{:,5}],2,size(model.param_properties,1))'),2));
0132 nb_param                = length(parameter_search_idx);
0133 delta_grad              = 1E-4*ones(length(model.parameter),1);
0134 parameterRef            = model.parameter;
0135 parameterRefTR          = zeros(length(parameterRef), 1);
0136 transfFunc.TR           = cell(1,nb_param);
0137 transfFunc.InvTR        = cell(1,nb_param);
0138 transfFunc.gradTR2OR    = cell(1,nb_param);
0139 <span class="keyword">for</span> i = 1 : nb_param
0140     idx                 = parameter_search_idx(i);
0141     [transfFunc.TR{i},transfFunc.InvTR{i},transfFunc.gradTR2OR{i},~] = <a href="parameter_transformation_fct.html" class="code" title="function [fct_TR,fct_inv_TR,grad_TR2OR,hessian_TR2OR]=parameter_transformation_fct(model,param_idx_loop)">parameter_transformation_fct</a>(model,idx);
0142     parameterRefTR(idx) = transfFunc.TR{i}(parameterRef(idx));
0143 <span class="keyword">end</span>
0144 parameterSearchInit   = parameterRef(parameter_search_idx);
0145 parameterSearchInitTR = parameterRefTR(parameter_search_idx);
0146 parameter             = parameterRef; 
0147 parameterTR           = parameterRefTR;
0148 model.parameterTR     = parameterTR;
0149 <span class="comment">%% Parameter name</span>
0150 name_idx_1=<span class="string">''</span>;
0151 name_idx_2=<span class="string">''</span>;
0152 <span class="keyword">for</span> i=parameter_search_idx'
0153     name_p1{i}=[model.param_properties{i,1}];
0154     <span class="keyword">if</span> ~isempty(model.param_properties{i,4})
0155         temp=model.param_properties{i,4}(1);
0156     <span class="keyword">else</span>
0157         temp=<span class="string">''</span>;
0158     <span class="keyword">end</span>
0159     name_p2{i}=[model.param_properties{i,2}, <span class="string">'|M'</span>, model.param_properties{i,3},<span class="string">'|'</span>,temp];
0160     name_idx_1=[name_idx_1  name_p1{i} repmat(<span class="string">' '</span>,[1,15-length(name_p1{i})]) <span class="string">' '</span>];
0161     name_idx_2=[name_idx_2  name_p2{i} repmat(<span class="string">' '</span>,[1,15-length(name_p2{i})]) <span class="string">' '</span>];  
0162 <span class="keyword">end</span>
0163 
0164 <span class="comment">%% Optimization process</span>
0165 logpdf              = zeros(1, maxIter);
0166 logpdfHist          = zeros(nb_param+1, maxIter * NminiBatch);
0167 idxMax_loop         = zeros(1, maxIter * NminiBatch);
0168 parameterSearch     = zeros(nb_param, maxIter);
0169 parameterSearchTR   = zeros(nb_param, maxIter);
0170 momentumTR          = zeros(nb_param, maxIter);
0171 RMSpropTR           = zeros(nb_param, maxIter);
0172 momentumTRhist      = zeros(nb_param, maxIter * NminiBatch);
0173 RMSpropTRhist       = zeros(nb_param, maxIter * NminiBatch);
0174 learningRate        = learningRateDefaut * ones(nb_param, maxIter * NminiBatch);
0175 gradientTR          = zeros(nb_param, maxIter * NminiBatch);
0176 hessianTR           = zeros(nb_param, maxIter * NminiBatch);
0177 zeroGradCount       = zeros(nb_param,1);
0178 paramMoveCount      = ones(nb_param,1);
0179 paramReset          = zeros(nb_param,1);
0180 metricVL            = zeros(1, maxIter);
0181 metricVLhist        = zeros(nb_param+1, maxIter * NminiBatch);
0182 mmtHessianTR        = zeros(nb_param, maxIter);
0183 mmtHessianTRhist    = zeros(nb_param, maxIter * NminiBatch);
0184 paramChange         = zeros(nb_param, maxIter);
0185 
0186 [metricVL(1),~, ~, logpdf(1)] = <a href="metricFct.html" class="code" title="function [metricVL, idxMaxM, logpdf_test, logpdf_train] = metricFct(data_train, data_test, model, option, parameterSearch, parameterSearchTR)">metricFct</a>(data_train, data_valid, model, misc, parameter(parameter_search_idx), parameterTR(parameter_search_idx));
0187 parameterSearch(:,1)    = parameterSearchInit;
0188 parameterSearchTR(:,1)  = parameterSearchInitTR;
0189 gradientTR(:,1)         = NaN(nb_param, 1);
0190 hessianTR(:,1)          = NaN(nb_param, 1);
0191 
0192 tic; <span class="comment">% time counter initialization</span>
0193 Nepoch      = 1;
0194 Nloop       = 0;
0195 time_loop   = 0;
0196 stop_loop = 0;
0197 <span class="keyword">if</span> disp_flag==1
0198     disp(<span class="string">' '</span>)
0199     disp([<span class="string">'               Nepoch #'</span> num2str(Nepoch)])
0200     disp([<span class="string">'               Metric: '</span> num2str(metricVL(1))])
0201     disp([<span class="string">'                       '</span> name_idx_2])
0202     disp([<span class="string">'      parameter names: '</span> name_idx_1])
0203     fprintf([<span class="string">'       initial values: '</span> repmat([<span class="string">'%#-+15.2e'</span> <span class="string">' '</span>],[1,length(parameterRef)]) <span class="string">'%#-+15.2e\n'</span>],parameterRef(parameter_search_idx))
0204     disp(<span class="string">' '</span>)
0205 <span class="keyword">end</span>
0206 <span class="keyword">while</span> Nepoch &lt;= maxIter &amp;&amp; time_loop &lt; timeLimit  
0207     Nepoch                  = Nepoch + 1;
0208     parameterSearch_loop    = parameter(parameter_search_idx);
0209     parameterSearchTR_loop  = parameterTR(parameter_search_idx);
0210     momentumTR_loop         = momentumTR(:,Nepoch - 1);
0211     RMSpropTR_loop          = RMSpropTR(:, Nepoch - 1);
0212     loopCounter1epoch       = 0;
0213     mmtHessianTR_loop       = mmtHessianTR(:, Nepoch-1);
0214     <span class="keyword">while</span> loopCounter1epoch &lt; NminiBatch
0215         Nloop       = Nloop + 1;
0216         loopCounter1epoch       = loopCounter1epoch + 1;
0217         <span class="comment">% Ramdomly chose the start point for mini batch</span>
0218         datapointbound          = length(data_train.values) - Ndata4miniBatch;
0219         rng(<span class="string">'shuffle'</span>)
0220         idxDStart               = randi([1, datapointbound], 1);
0221         idxData4miniBatch       = idxDStart:idxDStart+Ndata4miniBatch;
0222         [data_miniBatch, data_miniBatchTest]    = <a href="#_sub1" class="code" title="subfunction [data_train, data_valid] = dataSplit(data, idxTrain, alpha_split, varargin)">dataSplit</a>(data_train, idxData4miniBatch , alpha_split,<span class="string">'getTimeStepRef'</span>,0);  
0223         <span class="comment">% Parameter setup</span>
0224         model.parameter(parameter_search_idx)   = parameterSearch_loop;
0225         model.parameterTR(parameter_search_idx) = parameterSearchTR_loop; 
0226         delta_grad_loop                         = delta_grad(parameter_search_idx);
0227         param_idx_loop                          = parameter_search_idx;
0228         parfor p = 1:nb_param
0229             <span class="comment">% First and second derivatives computations</span>
0230             [~, GlogpdfTR_loop, HlogpdfTR_loop, ~] = <a href="logPosteriorPE.html" class="code" title="function [logpdf, Glogpdf, Hlogpdf, delta_grad] = logPosteriorPE(data, model, option, varargin)">logPosteriorPE</a>(data_miniBatch, model, misc,<span class="keyword">...</span>
0231                                                                                  <span class="string">'paramTR_index'</span>,param_idx_loop(p),<span class="keyword">...</span>
0232                                                                                  <span class="string">'stepSize4grad'</span>,delta_grad_loop(p));
0233             <span class="comment">% Store the 1st and 2nd derivatives</span>
0234             <span class="keyword">if</span> isnan(GlogpdfTR_loop) 
0235                 gradientTR(p, Nloop) = 0;
0236             <span class="keyword">else</span>
0237                gradientTR(p, Nloop)  = GlogpdfTR_loop;
0238             <span class="keyword">end</span>
0239             hessianTR(p, Nloop)     = HlogpdfTR_loop;
0240             <span class="comment">% Learning rate</span>
0241             <span class="keyword">if</span> strcmp(learningRate_mode,<span class="string">'hessian'</span>)
0242                 <span class="keyword">if</span> HlogpdfTR_loop &lt; 0
0243                     learningRate(p, Nloop)  = -1/HlogpdfTR_loop;
0244                 <span class="keyword">elseif</span> HlogpdfTR_loop &gt; 0
0245                     learningRate(p, Nloop)  = 1/HlogpdfTR_loop;
0246                 <span class="keyword">elseif</span> isnan(HlogpdfTR_loop)
0247                     learningRate(p, Nloop)=learningRateDefaut/(sqrt(paramMoveCount(p)));
0248                 <span class="keyword">end</span> 
0249             <span class="keyword">elseif</span> strcmp(learningRate_mode,<span class="string">'decay'</span>)
0250                 learningRate(p, Nloop)=learningRateDefaut/(sqrt(paramMoveCount(p)));
0251             <span class="keyword">elseif</span> strcmp(learningRate_mode,<span class="string">'defaut'</span>)
0252                 learningRate(p, Nloop) = learningRateDefaut;
0253             <span class="keyword">end</span>
0254         <span class="keyword">end</span>
0255         <span class="comment">% Learning rate constrains to avoid the exploding gradient problem.</span>
0256         <span class="comment">% See Recurent Neural Network (RNN) for futher details.</span>
0257        <span class="keyword">if</span> any(any(isnan(hessianTR(:,Nloop))))
0258             hessianTR(:,Nloop)=hessianDefaut;
0259         <span class="keyword">end</span>
0260         idx_lr = abs(hessianTR(:,Nloop))&lt;0.001;
0261         <span class="keyword">if</span> any(any(idx_lr))
0262             <span class="keyword">if</span> Nloop==1
0263                 hessianTR(idx_lr,Nloop)= hessianDefaut ;
0264             <span class="keyword">else</span>
0265                 hessianTR(idx_lr,Nloop)= hessianDefaut.*(sqrt(paramMoveCount(idx_lr)));
0266             <span class="keyword">end</span>
0267         <span class="keyword">end</span>
0268         idx_lr = abs(learningRate(:,Nloop))&gt;1000;
0269         <span class="keyword">if</span> any(any(idx_lr))
0270             <span class="keyword">if</span> Nloop==1
0271                 learningRate(idx_lr,Nloop)= learningRateDefaut;
0272             <span class="keyword">else</span>
0273                 learningRate(idx_lr,Nloop)= learningRate(idx_lr,Nloop)./sqrt(paramMoveCount(idx_lr));
0274             <span class="keyword">end</span>
0275         <span class="keyword">end</span>
0276         
0277         <span class="comment">% Parameter initialization in order to avoid the vanishing gradient</span>
0278         <span class="comment">% problem. See Recurent Neural Network (RNN) for further details.</span>
0279         <span class="keyword">if</span> any(abs(gradientTR(:, Nloop))&lt;1E-2)
0280             idxZeroGrad                     = abs(gradientTR(:, Nloop))&lt;1E-2;
0281             zeroGradCount(idxZeroGrad)      = zeroGradCount(idxZeroGrad)+1;
0282             zeroGradCount(~idxZeroGrad)     = 0;
0283             idxG                            = zeroGradCount&gt;0;
0284             momentumTR_loop(idxG)           = 0;
0285             RMSpropTR_loop(idxG)            = 0;
0286             gradientTR(idxG, Nloop)         = 0;
0287             paramMoveCount(idxG)            = 1;
0288             paramReset(idxG)                = paramReset(idxG) + 1;
0289             parameterRandom                 = mvnrnd(parameterSearchTR(idxG,1),stdInit*diag(ones(size(parameterSearchTR(idxG,1),1),1)));
0290             parameterSearchTR_loop(idxG)    = parameterRandom';
0291             <span class="keyword">for</span> p = 1:nb_param
0292                 parameterSearch_loop(p) = transfFunc.InvTR{p}(parameterSearchTR_loop(p));
0293             <span class="keyword">end</span>
0294             mmtHessianTR_loop(idxG) = 0;
0295         <span class="keyword">end</span>
0296         <span class="comment">% Grid search setup</span>
0297         [parameterSearchTRold_loop,<span class="keyword">...</span>
0298          momentumTRold_loop,<span class="keyword">...</span>
0299          RMSpropTRold_loop,<span class="keyword">...</span>
0300          gradientTRold_loop,<span class="keyword">...</span>
0301          learningRateTRold_loop,<span class="keyword">...</span>
0302          mmtHessianTRold_loop,<span class="keyword">...</span>
0303          hessianTRold_loop]         = <a href="#_sub6" class="code" title="subfunction [xM, momentumM, RMSpropM, gradM, learningRateM, mmtHessM, hessM]= paramGrid(x, momentum, RMSprop, grad, learningRate, mmtHess, hess)">paramGrid</a>(parameterSearchTR_loop, momentumTR_loop, RMSpropTR_loop, gradientTR(:, Nloop), learningRate(:, Nloop), mmtHessianTR_loop, hessianTR(:, Nloop));
0304         parameterSearchNew_loop     = zeros(nb_param, nb_param+1);
0305         parameterSearchTRNew_loop   = zeros(nb_param, nb_param+1);
0306         momentumTRNew_loop          = zeros(nb_param, nb_param+1); 
0307         RMSpropTRNew_loop           = zeros(nb_param, nb_param+1);
0308         funcInvTR                   = transfFunc.InvTR;
0309         mmtHessianTRNew_loop        = zeros(nb_param, nb_param+1);
0310         
0311         <span class="comment">% New parameter values computed using ADAM or MMT</span>
0312         <span class="keyword">if</span> strcmp(misc.optimizer,<span class="string">'ADAM'</span>)
0313              parfor p =1:nb_param+1
0314                  [parameterSearchNew_loop(:,p),<span class="keyword">...</span>
0315                   parameterSearchTRNew_loop(:,p),<span class="keyword">...</span>
0316                   momentumTRNew_loop(:,p),<span class="keyword">...</span>
0317                   RMSpropTRNew_loop(:,p)] = <a href="#_sub4" class="code" title="subfunction [xsearch, xsearchTR, momentumTR, RMSpropTR] = ADAM(xsearchTRprev, momentumTRprev, RMSpropTRprev, grad, step, beta_1, beta_2, epsilon, Niter, fctInvTR)">ADAM</a>(parameterSearchTRold_loop(:,p), momentumTRold_loop(:,p), RMSpropTRold_loop(:,p), gradientTRold_loop(:,p), learningRateTRold_loop(:, p), beta_1, beta_2, epsilon, paramMoveCount, funcInvTR);
0318              <span class="keyword">end</span>
0319         <span class="keyword">elseif</span> strcmp(misc.optimizer,<span class="string">'MMT'</span>)
0320             parfor p = 1:nb_param+1              
0321                 [parameterSearchNew_loop(:,p),<span class="keyword">...</span>
0322                  parameterSearchTRNew_loop(:,p),<span class="keyword">...</span>
0323                  momentumTRNew_loop(:,p)] = <a href="#_sub2" class="code" title="subfunction [xsearch, xsearchTR, momentumTR] = MMT(xsearchTRprev, momentumTRprev, grad, step, beta, fctInvTR)">MMT</a>(parameterSearchTRold_loop(:,p), momentumTRold_loop(:,p), gradientTRold_loop(:,p), learningRateTRold_loop(:, p), beta_1, funcInvTR );
0324             <span class="keyword">end</span>
0325         <span class="keyword">elseif</span> strcmp(misc.optimizer,<span class="string">'ADAMbeta'</span>)
0326             parfor p = 1:nb_param+1
0327                 [parameterSearchNew_loop(:,p),<span class="keyword">...</span>
0328                   parameterSearchTRNew_loop(:,p),<span class="keyword">...</span>
0329                   momentumTRNew_loop(:,p),<span class="keyword">...</span>
0330                   RMSpropTRNew_loop(:,p),<span class="keyword">...</span>
0331                   mmtHessianTRNew_loop(:,p)] = <a href="#_sub5" class="code" title="subfunction [xsearch, xsearchTR, momentumTR, RMSpropTR, mmtHessianTR] = ADAMbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, RMSpropTRprev, grad, hess, beta_1, beta_2, epsilon, Niter, fctInvTR)">ADAMbeta</a>(parameterSearchTRold_loop(:,p), momentumTRold_loop(:,p), mmtHessianTRold_loop(:,p), RMSpropTRold_loop(:,p), gradientTRold_loop(:,p), hessianTRold_loop(:,p), beta_1, beta_2, epsilon, paramMoveCount, funcInvTR);
0332             <span class="keyword">end</span>            
0333         <span class="keyword">elseif</span> strcmp(misc.optimizer,<span class="string">'MMTbeta'</span>)
0334             parfor p = 1:nb_param+1
0335                 [parameterSearchNew_loop(:,p),<span class="keyword">...</span>
0336                  parameterSearchTRNew_loop(:,p),<span class="keyword">...</span>
0337                  momentumTRNew_loop(:,p),<span class="keyword">...</span>
0338                  mmtHessianTRNew_loop(:,p)] = <a href="#_sub3" class="code" title="subfunction [xsearch, xsearchTR, momentumTR, mmtHessianTR] = MMTbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, grad, hess, beta, fctInvTR)">MMTbeta</a>(parameterSearchTRold_loop(:,p), momentumTRold_loop(:,p), mmtHessianTRold_loop(:,p), gradientTRold_loop(:,p), hessianTRold_loop(:,p), beta_1, funcInvTR );
0339             <span class="keyword">end</span>
0340         <span class="keyword">end</span>   
0341         <span class="comment">% Parameter selection for the mini batch</span>
0342         [metricVLhist(:,Nloop),idxMaxPC, ~, logpdfHist(:,Nloop)] = <a href="metricFct.html" class="code" title="function [metricVL, idxMaxM, logpdf_test, logpdf_train] = metricFct(data_train, data_test, model, option, parameterSearch, parameterSearchTR)">metricFct</a>(data_miniBatch, data_miniBatchTest, model, misc, parameterSearchNew_loop, parameterSearchTRNew_loop);
0343         <span class="keyword">if</span> strcmp(misc.metric_mode,<span class="string">'predCap'</span>)
0344             [idxMaxlogpdf,~]=nanmax(logpdfHist(:,Nloop));
0345             <span class="keyword">if</span> idxMaxlogpdf==idxMaxPC
0346                 idxMax_loop(Nloop)=1;
0347             <span class="keyword">else</span>
0348                 idxMax_loop(Nloop) = 0;
0349             <span class="keyword">end</span>
0350         <span class="keyword">else</span>
0351             idxMax_loop(Nloop) = NaN;
0352         <span class="keyword">end</span>
0353         <span class="comment">% Parameter update for the mini batch</span>
0354         momentumTR_loop  = momentumTRNew_loop(:,end);
0355         RMSpropTR_loop   = RMSpropTRNew_loop(:,end);
0356         mmtHessianTR_loop = mmtHessianTRNew_loop(:,end);
0357         <span class="keyword">if</span> idxMaxPC ~= nb_param+1
0358             idxMomentum                  = momentumTRNew_loop(:,idxMaxPC)~=0;
0359             momentumTR_loop(:)           = 0;
0360             RMSpropTR_loop(:)            = 0;
0361             momentumTR_loop(idxMomentum) = momentumTRNew_loop(idxMomentum,idxMaxPC);
0362             RMSpropTR_loop(idxMomentum)  = RMSpropTRNew_loop(idxMomentum,idxMaxPC);
0363             paramMoveCount(idxMomentum)  = paramMoveCount(idxMomentum)+1;
0364             mmtHessianTR_loop(idxMomentum)= mmtHessianTRNew_loop(idxMomentum,idxMaxPC);
0365         <span class="keyword">else</span>
0366             paramMoveCount = paramMoveCount+1;
0367         <span class="keyword">end</span>
0368         parameterSearchTR_loop  = parameterSearchTRNew_loop(:,idxMaxPC);
0369         parameterSearch_loop    = parameterSearchNew_loop(:,idxMaxPC);
0370         momentumTRhist(:,Nloop) = momentumTR_loop;
0371         RMSpropTRhist(:,Nloop)  = RMSpropTR_loop;
0372         mmtHessianTRhist(:,Nloop)= mmtHessianTR_loop;
0373     <span class="keyword">end</span>  
0374     <span class="comment">% Metric calculation for each epoch</span>
0375     [metricVL(Nepoch),~, ~, logpdf(Nepoch)] = <a href="metricFct.html" class="code" title="function [metricVL, idxMaxM, logpdf_test, logpdf_train] = metricFct(data_train, data_test, model, option, parameterSearch, parameterSearchTR)">metricFct</a>(data_train, data_valid, model, misc, parameterSearch_loop, parameterSearchTR_loop);    
0376     
0377     <span class="keyword">if</span> sign(metricVL(Nepoch-1))==-1
0378         TL_loop = termination_tolerance+1;
0379     <span class="keyword">else</span>
0380         TL_loop = termination_tolerance;
0381     <span class="keyword">end</span>
0382     
0383     <span class="comment">% Parameter update for the training set</span>
0384     <span class="keyword">if</span> or(metricVL(Nepoch) &gt; metricVL(Nepoch-1)*TL_loop,logpdf(Nepoch) &gt; logpdf(Nepoch-1)*TL_loop)
0385         parameterSearch(:,Nepoch)           = parameterSearch_loop;
0386         parameterSearchTR(:,Nepoch)         = parameterSearchTR_loop;
0387         parameter(parameter_search_idx)     = parameterSearch(:,Nepoch);  
0388         parameterTR(parameter_search_idx)   = parameterSearchTR(:,Nepoch); 
0389         momentumTR(:,Nepoch)                = momentumTR_loop;
0390         RMSpropTR(:,Nepoch)                 = RMSpropTR_loop;
0391         stop_loop = 0;
0392         mmtHessianTR(:,Nepoch)              = mmtHessianTR_loop;
0393     <span class="keyword">else</span>
0394         stop_loop = stop_loop + 1;
0395         <span class="keyword">if</span> stop_loop &gt; 3
0396              stop_loop=0;
0397             [metricVL(Nepoch),idxMaxEpochs]   = nanmax(metricVL);
0398             parameterSearch(:,Nepoch)         = parameterSearch(:,idxMaxEpochs);
0399             parameterSearchTR(:,Nepoch)       = parameterSearchTR(:,idxMaxEpochs);  
0400             parameter(parameter_search_idx)   = parameterSearch(:,idxMaxEpochs);
0401             parameterTR(parameter_search_idx) = parameterSearchTR(:,idxMaxEpochs);
0402             momentumTR(:,Nepoch)              = momentumTR(:,idxMaxEpochs);
0403             RMSpropTR(:,Nepoch)               = RMSpropTR(:,idxMaxEpochs);
0404             logpdf(Nepoch)                    = logpdf(idxMaxEpochs); 
0405             metricVL(Nepoch)                  = metricVL(idxMaxEpochs); 
0406             momentumTRhist(:,Nloop)           = momentumTR(:,Nepoch);
0407             RMSpropTRhist(:,Nloop)            = RMSpropTR(:,Nepoch);
0408             mmtHessianTR(:, Nepoch)           = mmtHessianTR(:,idxMaxEpochs);
0409         <span class="keyword">else</span>
0410             parameterSearch(:,Nepoch)           = parameterSearch(:,Nepoch-1);
0411             parameterSearchTR(:,Nepoch)         = parameterSearchTR(:,Nepoch-1);  
0412             parameter(parameter_search_idx)     = parameterSearch(:,Nepoch-1);
0413             parameterTR(parameter_search_idx)   = parameterSearchTR(:,Nepoch-1); 
0414             momentumTR(:,Nepoch)                = momentumTR(:,1);
0415             RMSpropTR(:,Nepoch)                 = RMSpropTR(:,1);
0416             logpdf(Nepoch)                      = logpdf(Nepoch-1);
0417             metricVL(Nepoch)                    = metricVL(Nepoch-1);       
0418             momentumTRhist(:,Nloop)             = momentumTR(:,Nepoch);
0419             RMSpropTRhist(:,Nloop)              = RMSpropTR(:,Nepoch);
0420             mmtHessianTR(:, Nepoch)             = mmtHessianTR(:,1);
0421         <span class="keyword">end</span>   
0422     <span class="keyword">end</span>
0423     paramChange(:,Nepoch)  = (parameterSearch(:,Nepoch)-parameterSearch(:,Nepoch-1));
0424     disp(<span class="string">' '</span>)
0425     disp(<span class="string">'--------------------------'</span>)
0426     disp([<span class="string">'    Epoch #'</span> num2str(Nepoch)])
0427     disp([<span class="string">'            Metric: '</span> num2str(metricVL(Nepoch))])
0428     disp([<span class="string">'   parameter names: '</span> name_idx_2])
0429     fprintf([<span class="string">'    current values: '</span> repmat([<span class="string">'%#-+15.2e'</span> <span class="string">' '</span>],[1,length(parameter(parameter_search_idx))-1]) <span class="string">'%#-+15.2e\n'</span>,<span class="keyword">...</span>
0430              <span class="string">'      param change: '</span> repmat([<span class="string">'%#-+15.2e'</span> <span class="string">' '</span>],[1,length(parameter(parameter_search_idx))-1]) <span class="string">'%#-+15.2e\n'</span>,<span class="keyword">...</span>
0431              <span class="string">'  initialize param: '</span> repmat([<span class="string">'%#-+15.2e'</span> <span class="string">' '</span>],[1,length(parameter(parameter_search_idx))-1]) <span class="string">'%#-+15.2e\n'</span>],<span class="keyword">...</span>
0432              parameter(parameter_search_idx),<span class="keyword">...</span>
0433              paramChange(:,Nepoch),<span class="keyword">...</span>
0434              paramReset);   
0435     time_loop=toc; 
0436 <span class="keyword">end</span>
0437 
0438 <span class="comment">%% Selection of the optimal parameters corresponding to the best log-post</span>
0439 [metricVLmax,idxmax]                    = nanmax(metricVL(1:Nepoch));
0440 parameterSearch_opt                     = parameterSearch(:,idxmax);
0441 parameterSearchTR_opt                   = parameterSearchTR(:,idxmax);
0442 parameter_opt                           = parameter;
0443 parameter_opt(parameter_search_idx)     = parameterSearch_opt;
0444 parameterTR_opt                         = parameterTR;
0445 parameterTR_opt(parameter_search_idx)   = parameterSearchTR_opt;
0446 
0447 <span class="comment">%% Output</span>
0448 optim.parameter_opt         = parameter_opt;
0449 optim.parameterTR_opt       = parameterTR_opt;
0450 optim.metricVLmax           = metricVLmax;
0451 optim.metricVL              = metricVL;
0452 optim.logpdf                = logpdf;
0453 optim.log_lik               = nanmax(logpdf); 
0454 optim.Nepoch                = Nepoch;
0455 optim.converged             = 0;
0456 optim.gradientTR            = gradientTR;
0457 optim.learningRate          = learningRate;
0458 optim.hessianTR             = hessianTR;
0459 optim.parameterSearch       = parameterSearch;
0460 optim.parameterSearchTR     = parameterSearchTR;
0461 optim.momentumTR            = momentumTR;
0462 optim.RMSpropTR             = RMSpropTR;
0463 optim.Ndata4miniBatch       = Ndata4miniBatch;
0464 optim.idxMax_loop           = idxMax_loop; 
0465 optim.optim_mode            = misc.optim_mode;
0466 optim.beta_1                = beta_1;
0467 optim.beta_2                = beta_2;
0468 optim.epsilon               = epsilon;
0469 optim.learningRateDefaut    = learningRateDefaut;
0470 optim.data_train            = data_train;
0471 optim.data_valid            = data_valid;
0472 optim.data                  = data;
0473 optim.data_train            = data_valid;
0474 optim.misc                = misc;
0475 <span class="keyword">end</span>
0476 
0477 <a name="_sub1" href="#_subfunctions" class="code">function [data_train, data_valid] = dataSplit(data, idxTrain, alpha_split, varargin)</a>
0478 getTimeStepRef = 1;
0479 timeStepMean   = 0;
0480 args    = varargin;
0481 nargs   = length(varargin);
0482 tol     = 1e-4; 
0483 <span class="keyword">for</span> n = 1:2:nargs
0484     <span class="keyword">switch</span> args{n}
0485         <span class="keyword">case</span> <span class="string">'getTimeStepRef'</span>,   getTimeStepRef = args{n+1};
0486         <span class="keyword">case</span> <span class="string">'timeStepMean'</span>,     timeStepMean   = args{n+1};
0487         <span class="keyword">otherwise</span>, error([<span class="string">'Unrecognized argument'</span> args{n}]) 
0488     <span class="keyword">end</span>
0489 <span class="keyword">end</span>
0490 dataTrainingProcess.values      = data.values(idxTrain,:);
0491 dataTrainingProcess.timestamps  = data.timestamps(idxTrain,:);
0492 NdataTrainingProcess            = size(dataTrainingProcess.values ,1);
0493 NdataValid                      = round(alpha_split*NdataTrainingProcess);
0494 
0495 data_train.values       = dataTrainingProcess.values(1:NdataTrainingProcess-NdataValid,:);
0496 data_train.timestamps   = dataTrainingProcess.timestamps(1:NdataTrainingProcess-NdataValid,:);
0497 data_train.nb_steps     = size(data_train.values,1);
0498 
0499 data_valid              = dataTrainingProcess;
0500 data_valid.nb_steps     = size(data_valid.values,1);
0501 <span class="keyword">if</span> getTimeStepRef==1
0502     data_train.dt_steps         = diff(data_train.timestamps);
0503     <span class="keyword">if</span> timeStepMean==1
0504         data_train.dt_ref = mean(data_train.dt_steps);
0505     <span class="keyword">else</span>
0506         unique_dt_steps             = uniquetol(data_train.dt_steps,tol);
0507         counts_dt_steps             = [unique_dt_steps,histc(data_train.dt_steps(:),unique_dt_steps)];
0508         data_train.dt_ref           = counts_dt_steps(find(counts_dt_steps(:,2)==max(counts_dt_steps(:,2)),1,<span class="string">'first'</span>),1);
0509     <span class="keyword">end</span>
0510     data_train.dt_steps = [data_train.dt_ref;data_train.dt_steps];
0511     data_valid.dt_steps = diff(data_valid.timestamps);   
0512     data_valid.dt_ref   = data_train.dt_ref;
0513     data_valid.dt_steps = [data_valid.dt_ref; data_valid.dt_steps];
0514 <span class="keyword">else</span>
0515     data_train.dt_ref   = data.dt_ref;
0516     data_train.dt_steps = data.dt_steps(1:NdataTrainingProcess-NdataValid,:); 
0517     data_valid.dt_ref   = data.dt_ref;
0518     data_valid.dt_steps = data.dt_steps(1:NdataTrainingProcess,:);
0519 <span class="keyword">end</span>
0520 <span class="keyword">end</span>
0521 
0522 <a name="_sub2" href="#_subfunctions" class="code">function [xsearch, xsearchTR, momentumTR] = MMT(xsearchTRprev, momentumTRprev, grad, step, beta, fctInvTR)</a>
0523 xsearch     = zeros(length(xsearchTRprev), 1);
0524 momentumTR   = beta*momentumTRprev + (1-beta)*grad;
0525 xsearchTR   = xsearchTRprev + step.*momentumTR;
0526 <span class="keyword">for</span> p = 1:length(xsearchTRprev)
0527     xsearch(p) =  fctInvTR{p}(xsearchTR(p));
0528 <span class="keyword">end</span>
0529 <span class="keyword">end</span>
0530 
0531 <a name="_sub3" href="#_subfunctions" class="code">function [xsearch, xsearchTR, momentumTR, mmtHessianTR] = MMTbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, grad, hess, beta, fctInvTR)</a>
0532 xsearch      = zeros(length(xsearchTRprev), 1);
0533 momentumTR   = beta*momentumTRprev + (1-beta)*grad;
0534 mmtHessianTR = beta*mmtHessianTRprev + (1-beta)*abs(hess);
0535 step         = 1./mmtHessianTR;
0536 step(step==Inf)= 0;
0537 xsearchTR    = xsearchTRprev + step.*momentumTR;
0538 <span class="keyword">for</span> p = 1:length(xsearchTRprev)
0539     xsearch(p) =  fctInvTR{p}(xsearchTR(p));
0540 <span class="keyword">end</span>
0541 <span class="keyword">end</span>
0542 
0543 <a name="_sub4" href="#_subfunctions" class="code">function [xsearch, xsearchTR, momentumTR, RMSpropTR] = ADAM(xsearchTRprev, momentumTRprev, RMSpropTRprev, grad, step, beta_1, beta_2, epsilon, Niter, fctInvTR)</a>
0544 xsearch         = zeros(length(xsearchTRprev), 1);
0545 momentumTR      = beta_1*momentumTRprev + (1-beta_1)*grad;
0546 RMSpropTR       = beta_2*RMSpropTRprev + (1-beta_2)*grad.^2;
0547 momentumTRcorr  = momentumTR./(1-beta_1.^Niter);
0548 RMSpropTRcorr   = RMSpropTR./(1-beta_2.^Niter);
0549 xsearchTR       = xsearchTRprev + step.*momentumTRcorr./(sqrt(RMSpropTRcorr)+epsilon);
0550 <span class="keyword">for</span> p = 1:length(xsearchTRprev)
0551     xsearch(p) =  fctInvTR{p}(xsearchTR(p));
0552 <span class="keyword">end</span>
0553 <span class="keyword">end</span>
0554 
0555 <a name="_sub5" href="#_subfunctions" class="code">function [xsearch, xsearchTR, momentumTR, RMSpropTR, mmtHessianTR] = ADAMbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, RMSpropTRprev, grad, hess, beta_1, beta_2, epsilon, Niter, fctInvTR)</a>
0556 xsearch         = zeros(length(xsearchTRprev), 1);
0557 momentumTR      = beta_1*momentumTRprev + (1-beta_1)*grad;
0558 mmtHessianTR    = beta_1*mmtHessianTRprev + (1-beta_1)*abs(hess);
0559 RMSpropTR       = beta_2*RMSpropTRprev + (1-beta_2)*grad.^2;
0560 momentumTRcorr  = momentumTR./(1-beta_1.^Niter);
0561 mmtHessTRcorr   = mmtHessianTR./(1-beta_1.^Niter);
0562 RMSpropTRcorr   = RMSpropTR./(1-beta_2.^Niter);
0563 step            = 1./mmtHessTRcorr;
0564 step(step==Inf) = 0;
0565 xsearchTR       = xsearchTRprev + step.*momentumTRcorr./(sqrt(RMSpropTRcorr)+epsilon);
0566 <span class="keyword">for</span> p = 1:length(xsearchTRprev)
0567     xsearch(p) =  fctInvTR{p}(xsearchTR(p));
0568 <span class="keyword">end</span>
0569 <span class="keyword">end</span>
0570 
0571 <a name="_sub6" href="#_subfunctions" class="code">function [xM, momentumM, RMSpropM, gradM, learningRateM, mmtHessM, hessM]= paramGrid(x, momentum, RMSprop, grad, learningRate, mmtHess, hess)</a>
0572 d             = length(x);
0573 xM            = repmat(x,1,d+1);
0574 momentumM     = [diag(momentum) momentum];
0575 mmtHessM      = [diag(mmtHess) mmtHess];
0576 RMSpropM      = [diag(RMSprop) RMSprop];
0577 gradM         = [diag(grad) grad];
0578 hessM         = [diag(hess) hess];
0579 learningRateM = [diag(learningRate) learningRate];
0580 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Mon 18-Jun-2018 12:19:50 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>