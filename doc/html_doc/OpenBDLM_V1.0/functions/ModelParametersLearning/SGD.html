<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
  <title>Description of SGD</title>
  <meta name="keywords" content="SGD">
  <meta name="description" content="INPUTS:">
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
  <meta name="generator" content="m2html v1.5 &copy; 2003-2005 Guillaume Flandin">
  <meta name="robots" content="index, follow">
  <link type="text/css" rel="stylesheet" href="../../../m2html.css">
</head>
<body>
<a name="_top"></a>
<div><a href="../../../index.html">Home</a> &gt;  <a href="../../index.html">OpenBDLM_V1.0</a> &gt; <a href="#">functions</a> &gt; <a href="index.html">ModelParametersLearning</a> &gt; SGD.m</div>

<!--<table width="100%"><tr><td align="left"><a href="../../../index.html"><img alt="<" border="0" src="../../../left.png">&nbsp;Master index</a></td>
<td align="right"><a href="index.html">Index for OpenBDLM_V1.0/functions/ModelParametersLearning&nbsp;<img alt=">" border="0" src="../../../right.png"></a></td></tr></table>-->

<h1>SGD
</h1>

<h2><a name="_name"></a>PURPOSE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>INPUTS:</strong></div>

<h2><a name="_synopsis"></a>SYNOPSIS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="box"><strong>function [optim, model] = SGD(data, model, misc, varargin) </strong></div>

<h2><a name="_description"></a>DESCRIPTION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre class="comment"> INPUTS:
 maxEpoch             - Maximal number of epochs. Defaut is 30.
 Ndata4miniBatch           - Size of mini batch. Defaut is 0.2 of the training
                             data.
 alpha_split               - Portion of the validation data. Defaut is 0.3.
 optim_mode                - Optimization method can be either MLE or MAP.
                             Defaut is MLE.
 optimizer                 - optimizer can be Adaptive Moment Estimation (ADAM)
                             or Momentum (MMT). There are also 2
                             alternatives; ADAMbeta, MMTbeta. Defaut is MMT.
                             ADAM    : See Kingma and Lei Ba (2017)
                             ADAMbeta: See Schaul et al. (2013)
 metric_mode               - metric can be either prediction capacity (predCap)
                             or log-likelihood (logpdf). Defaut is predCap.
 learningRateDefaut        - Manual tuning learning rate
 learningRate_mode         - Learning rate can be hessian, decay, or defaut.
                             hessian -&gt; learningRate = 1/abs(hessian)
                             decay   -&gt; learningRate = learningRateDefaut/sqrt(t)
                             defaut  -&gt; learningRate = learningRateDefaut.
                             Defaut is hessian.
 beta_1                    - Average coefficient for the 1st moment (gradient).
                             Defaut is 0.9.
 beta_2                    - Average coefficient for the 2nd moment
                             (gradient^2). Defaut is 0.999.
 epsilon                   - Coefficient in ADAM. Defaut is 1E-8.
 termination_tolerance     - Moving control coefficient for metric. Defaut
                             is 0.95.</pre></div>

<!-- crossreference -->
<h2><a name="_cross"></a>CROSS-REFERENCE INFORMATION <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
This function calls:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="logPosteriorPE.html" class="code" title="function [logpdf, Glogpdf, Hlogpdf, delta_grad] = logPosteriorPE(data, model, misc, varargin)">logPosteriorPE</a>	INPUTS:</li><li><a href="metricFct.html" class="code" title="function [metricVL, idxMaxM, logpdf_test, logpdf_train] = metricFct(data_train, data_test, model, misc, parameterSearch, parameterSearchTR)">metricFct</a>	</li><li><a href="parameter_transformation_fct.html" class="code" title="function [fct_TR,fct_inv_TR,grad_TR2OR,hessian_TR2OR]=parameter_transformation_fct(model,param_idx_loop)">parameter_transformation_fct</a>	</li><li><a href="../../../OpenBDLM_V1.0/functions/Others/day2sampleIndex.html" class="code" title="function [Index]=day2sampleIndex(day, timestamps)">day2sampleIndex</a>	DAY2SAMPLEINDEX Convert a number of days to timestamp index</li><li><a href="../../../OpenBDLM_V1.0/functions/Others/readParameterProperties.html" class="code" title="function [arrayOut]=readParameterProperties(cellIn, Position)">readParameterProperties</a>	READPARAMETERPROPERTIES Extract some columns of a cell array</li><li><a href="../../../OpenBDLM_V1.0/functions/Others/writeParameterProperties.html" class="code" title="function [cellOut]=writeParameterProperties(cellIn, arrayIn, Position)">writeParameterProperties</a>	WRITEPARAMETERPROPERTIES Write updated model parameter properties</li></ul>
This function is called by:
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="learnModelParameters.html" class="code" title="function [data, model, estimation, misc]=learnModelParameters(data, model, estimation, misc, varargin)">learnModelParameters</a>	LEARNMODELPARAMETERS Learn Bayesian dynamic linear model parameters</li></ul>
<!-- crossreference -->

<h2><a name="_subfunctions"></a>SUBFUNCTIONS <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<ul style="list-style-image:url(../../../matlabicon.gif)">
<li><a href="#_sub1" class="code">function [data_train, data_valid] = dataSplit(data, idxTrain, alpha_split, varargin)</a></li><li><a href="#_sub2" class="code">function [xsearch, xsearchTR, momentumTR] = MMT(xsearchTRprev, momentumTRprev, grad, step, beta, fctInvTR)</a></li><li><a href="#_sub3" class="code">function [xsearch, xsearchTR, momentumTR, mmtHessianTR] = MMTbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, grad, hess, beta, fctInvTR)</a></li><li><a href="#_sub4" class="code">function [xsearch, xsearchTR, momentumTR, RMSpropTR] = ADAM(xsearchTRprev, momentumTRprev, RMSpropTRprev, grad, step, beta_1, beta_2, epsilon, Niter, fctInvTR)</a></li><li><a href="#_sub5" class="code">function [xsearch, xsearchTR, momentumTR, RMSpropTR, mmtHessianTR] = ADAMbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, RMSpropTRprev, grad, hess, beta_1, beta_2, epsilon, Niter, fctInvTR)</a></li><li><a href="#_sub6" class="code">function [xM, momentumM, RMSpropM, gradM, learningRateM, mmtHessM, hessM]= paramGrid(x, momentum, RMSprop, grad, learningRate, mmtHess, hess)</a></li></ul>

<h2><a name="_source"></a>SOURCE CODE <a href="#_top"><img alt="^" border="0" src="../../../up.png"></a></h2>
<div class="fragment"><pre>0001 <a name="_sub0" href="#_subfunctions" class="code">function [optim, model] = SGD(data, model, misc, varargin)</a>
0002 <span class="comment">% INPUTS:</span>
0003 <span class="comment">% maxEpoch             - Maximal number of epochs. Defaut is 30.</span>
0004 <span class="comment">% Ndata4miniBatch           - Size of mini batch. Defaut is 0.2 of the training</span>
0005 <span class="comment">%                             data.</span>
0006 <span class="comment">% alpha_split               - Portion of the validation data. Defaut is 0.3.</span>
0007 <span class="comment">% optim_mode                - Optimization method can be either MLE or MAP.</span>
0008 <span class="comment">%                             Defaut is MLE.</span>
0009 <span class="comment">% optimizer                 - optimizer can be Adaptive Moment Estimation (ADAM)</span>
0010 <span class="comment">%                             or Momentum (MMT). There are also 2</span>
0011 <span class="comment">%                             alternatives; ADAMbeta, MMTbeta. Defaut is MMT.</span>
0012 <span class="comment">%                             ADAM    : See Kingma and Lei Ba (2017)</span>
0013 <span class="comment">%                             ADAMbeta: See Schaul et al. (2013)</span>
0014 <span class="comment">% metric_mode               - metric can be either prediction capacity (predCap)</span>
0015 <span class="comment">%                             or log-likelihood (logpdf). Defaut is predCap.</span>
0016 <span class="comment">% learningRateDefaut        - Manual tuning learning rate</span>
0017 <span class="comment">% learningRate_mode         - Learning rate can be hessian, decay, or defaut.</span>
0018 <span class="comment">%                             hessian -&gt; learningRate = 1/abs(hessian)</span>
0019 <span class="comment">%                             decay   -&gt; learningRate = learningRateDefaut/sqrt(t)</span>
0020 <span class="comment">%                             defaut  -&gt; learningRate = learningRateDefaut.</span>
0021 <span class="comment">%                             Defaut is hessian.</span>
0022 <span class="comment">% beta_1                    - Average coefficient for the 1st moment (gradient).</span>
0023 <span class="comment">%                             Defaut is 0.9.</span>
0024 <span class="comment">% beta_2                    - Average coefficient for the 2nd moment</span>
0025 <span class="comment">%                             (gradient^2). Defaut is 0.999.</span>
0026 <span class="comment">% epsilon                   - Coefficient in ADAM. Defaut is 1E-8.</span>
0027 <span class="comment">% termination_tolerance     - Moving control coefficient for metric. Defaut</span>
0028 <span class="comment">%                             is 0.95.</span>
0029 
0030 <span class="comment">% OUTPUTS:</span>
0031 <span class="comment">% optim.parameter_opt       - optimal parameters in original space.</span>
0032 <span class="comment">% optim.parameterTR_opt     - optimal parameters in transformed space.</span>
0033 <span class="comment">% model</span>
0034 
0035 <span class="comment">% TIPS:</span>
0036 <span class="comment">% - THE HEAVILY COMPUTATIONNAL RESOURCE IS REQUIRED FOR THIS ALGORITHM</span>
0037 <span class="comment">% - If optimizer is ADAM, learningRate_mode should be either decay or</span>
0038 <span class="comment">%   defaut. Also, ADAM work well with a small training set.</span>
0039 <span class="comment">% - If optimizer is MMT, learningRate_mode should be hessian.</span>
0040 <span class="comment">%   MMT converge fast to the maximum point, but it is require a decent</span>
0041 <span class="comment">%   amount of data such &gt; 2000 (data point).</span>
0042 <span class="comment">% - ADAM     -&gt; learningRate_mode = defaut</span>
0043 <span class="comment">% - ADAMbeta -&gt; learningRate_mode = hessian</span>
0044 <span class="comment">% - MMT      -&gt; learningRate_mode = hessian</span>
0045 <span class="comment">% - MMTbeta  -&gt; learningRate_mode = hessian</span>
0046 <span class="comment">% - predCap is recommended as the metric for optimization process</span>
0047 <span class="comment">% - The hyperparameter such as alpha_split, learningRateDefaut, stdInit,</span>
0048 <span class="comment">%   beta_1, beta_2, epsilon, termination_tolorance are recommended to set</span>
0049 <span class="comment">%   as defaut for an effective performance.</span>
0050 
0051 <span class="comment">%% Get options from misc structure</span>
0052 
0053 trainingPeriod=misc.options.trainingPeriod;
0054 isMAP=misc.options.isMAP;
0055 isPredCap=misc.options.isPredCap;
0056 maxTime = misc.options.maxTime;
0057 isMute=misc.options.isMute;
0058 
0059 maxEpochs = misc.options.maxEpochs;
0060 alpha_split= misc.options.SplitPercent/100;
0061 MiniBatchPercent = misc.options.MiniBatchSizePercent/100;
0062 optimizer = misc.options.Optimizer;
0063 termination_tolerance = misc.options.SGTerminationTolerance/100;
0064 
0065 <span class="keyword">if</span> isMAP
0066     optim_mode=<span class="string">'MAP'</span>;
0067 <span class="keyword">else</span>
0068     optim_mode=<span class="string">'MLE'</span>;
0069 <span class="keyword">end</span>
0070 
0071 
0072 <span class="keyword">if</span> isPredCap
0073     metric_mode=<span class="string">'predCap'</span>;
0074 <span class="keyword">else</span>
0075     metric_mode=<span class="string">'logpdf'</span>;
0076 <span class="keyword">end</span>
0077 
0078 <span class="comment">%% Defaut values</span>
0079 <span class="comment">%warning('SGD ALGORITHM IS RECOMMENDED TO RUN ON MULTI-PROCESSORS')</span>
0080 <span class="comment">%alpha_split             = 0.3;</span>
0081 <span class="comment">%optimizer               = 'MMT';</span>
0082 learningRate_mode       = <span class="string">'hessian'</span>;
0083 learningRateDefaut      = 5E-3;
0084 stdInit                 = 0.5;
0085 beta_1                  = 0.9;
0086 beta_2                  = 0.999;
0087 epsilon                 = 1E-8;
0088 <span class="comment">%termination_tolerance   = 0.95;</span>
0089 hessianDefaut           = 1000;
0090 
0091 <span class="comment">% Set fileID for logfile</span>
0092 <span class="keyword">if</span> misc.internalVars.isQuiet
0093     <span class="comment">% output message in logfile</span>
0094     fileID=fopen(misc.internalVars.logFileName, <span class="string">'a'</span>);
0095 <span class="keyword">else</span>
0096     <span class="comment">% output message on screen and logfile using diary command</span>
0097     fileID=1;
0098 <span class="keyword">end</span>
0099 
0100 
0101 <span class="comment">%% Read model parameter properties</span>
0102 <span class="comment">% Current model parameters</span>
0103 idx_pvalues=size(model.param_properties,2)-1;
0104 idx_pref= size(model.param_properties,2);
0105 
0106 [arrayOut]=<span class="keyword">...</span>
0107     <a href="../../../OpenBDLM_V1.0/functions/Others/readParameterProperties.html" class="code" title="function [arrayOut]=readParameterProperties(cellIn, Position)">readParameterProperties</a>(model.param_properties, [idx_pvalues, idx_pref]);
0108 
0109 parameter= arrayOut(:,1);
0110 p_ref=arrayOut(:,2);
0111 
0112 <span class="comment">% Assign model.parameters</span>
0113 model.parameter=parameter;
0114 model.p_ref=p_ref;
0115 
0116 <span class="comment">%% Data</span>
0117 <span class="comment">% Get training period</span>
0118 training_start_idx = <a href="../../../OpenBDLM_V1.0/functions/Others/day2sampleIndex.html" class="code" title="function [Index]=day2sampleIndex(day, timestamps)">day2sampleIndex</a>(trainingPeriod(1), data.timestamps);
0119 training_end_idx = <a href="../../../OpenBDLM_V1.0/functions/Others/day2sampleIndex.html" class="code" title="function [Index]=day2sampleIndex(day, timestamps)">day2sampleIndex</a>(trainingPeriod(2), data.timestamps);
0120 
0121 misc.training_start_idx = training_start_idx;
0122 misc.training_end_idx =  training_end_idx;
0123 
0124 idxTrain                    = misc.training_start_idx:misc.training_end_idx;
0125 [data_train, data_valid]    = <a href="#_sub1" class="code" title="subfunction [data_train, data_valid] = dataSplit(data, idxTrain, alpha_split, varargin)">dataSplit</a>(data, idxTrain, alpha_split);
0126 data.dt_ref                 = data_train.dt_ref;
0127 data.dt_steps(1)            = data.dt_ref;
0128 Ndata4miniBatch             = round(MiniBatchPercent*size(data_valid.values,1));
0129 NminiBatch                  = ceil(length(data_train.values)/Ndata4miniBatch);
0130 <span class="keyword">if</span> NminiBatch&lt;2
0131     NminiBatch=1;
0132 <span class="keyword">end</span>
0133 <span class="comment">%% If provided, employ user-specific arguments</span>
0134 args    = varargin;
0135 nargs   = length(varargin);
0136 <span class="keyword">for</span> n = 1:2:nargs
0137     <span class="keyword">switch</span> args{n}
0138         <span class="comment">%case 'validationSet',       alpha_split         = args{n+1};</span>
0139         <span class="keyword">case</span> <span class="string">'learningRate_mode'</span>,   learningRate_mode   = args{n+1};
0140         <span class="keyword">case</span> <span class="string">'learningRate'</span>,        learningRateDefaut  = args{n+1};
0141         <span class="keyword">case</span> <span class="string">'beta_1'</span>,              beta_1              = args{n+1};
0142         <span class="keyword">case</span> <span class="string">'beta_2'</span>,              beta_2              = args{n+1};
0143         <span class="keyword">case</span> <span class="string">'epsilon'</span>,             epsilon             = args{n+1};
0144         <span class="keyword">case</span> <span class="string">'termination_tolerance'</span>,termination_tolerance = args{n+1};
0145         <span class="keyword">otherwise</span>, error([<span class="string">'unrecognized argument'</span> args{n}])
0146     <span class="keyword">end</span>
0147 <span class="keyword">end</span>
0148 
0149 disp([<span class="string">'     Learning model parameters '</span>, <span class="keyword">...</span>
0150     <span class="string">'(Stochastic gradient descent) ...'</span>])
0151 
0152 <span class="keyword">if</span> ~isMute
0153     <span class="comment">%% Diaplay analysis parameters</span>
0154     fprintf(fileID, <span class="string">'\n'</span>);
0155     fprintf(fileID, <span class="string">'    \\Start SGD algorithm (finite difference method)\n'</span>);
0156     fprintf(fileID, <span class="string">'\n'</span>);
0157     fprintf(fileID, <span class="string">'      Optimization mode                                       %s\n'</span>, optim_mode);
0158     fprintf(fileID, <span class="string">'      Optimizer                                               %s\n'</span>, optimizer);
0159     fprintf(fileID, <span class="string">'      Metric                                                  %s\n'</span>, metric_mode);
0160     fprintf(fileID, <span class="string">'      Learning Rate mode                                      %s\n'</span>, learningRate_mode);
0161     fprintf(fileID, <span class="string">'      Training period:                                        %s - %s [days]\n'</span>, num2str(trainingPeriod(1)), num2str(trainingPeriod(2)) );
0162     fprintf(fileID, <span class="string">'      Validation set portion:                                 %s [%%]\n'</span>, num2str(alpha_split*100) );
0163     fprintf(fileID, <span class="string">'      Training set:                                           %s [data points]\n'</span>, num2str(size(data_train.values,1)));
0164     fprintf(fileID, <span class="string">'      Validation set:                                         %s [data points]\n'</span>, num2str(size(data_valid.values,1)-size(data_train.values,1)));
0165     fprintf(fileID, <span class="string">'      Mini batch:                                             %s [data points]\n'</span>, num2str(Ndata4miniBatch));
0166     fprintf(fileID, <span class="string">'      Number of max epoch:                                    %s+1 [epochs]\n'</span>, num2str(maxEpochs));
0167     fprintf(fileID, <span class="string">'      Total time limit for calibration:                       %s [min]\n'</span>, num2str(maxTime));
0168     fprintf(fileID, <span class="string">'\n'</span>);
0169 <span class="comment">%     fprintf(fileID, '    ...in progress\n');</span>
0170 <span class="comment">%     fprintf(fileID, '\n');</span>
0171 <span class="keyword">end</span>
0172 <span class="comment">%% Initialize transformed model parameters</span>
0173 <span class="comment">%Identify parameters to be optimized</span>
0174 parameter_search_idx    = find(~all(isnan(reshape([model.param_properties{:,5}],2,size(model.param_properties,1))'),2));
0175 nb_param                = length(parameter_search_idx);
0176 delta_grad              = 1E-4*ones(length(model.parameter),1);
0177 parameterRef            = model.parameter;
0178 parameterRefTR          = zeros(length(parameterRef), 1);
0179 transfFunc.TR           = cell(1,nb_param);
0180 transfFunc.InvTR        = cell(1,nb_param);
0181 transfFunc.gradTR2OR    = cell(1,nb_param);
0182 <span class="keyword">for</span> i = 1 : nb_param
0183     idx                 = parameter_search_idx(i);
0184     [transfFunc.TR{i},transfFunc.InvTR{i},transfFunc.gradTR2OR{i},~] = <a href="parameter_transformation_fct.html" class="code" title="function [fct_TR,fct_inv_TR,grad_TR2OR,hessian_TR2OR]=parameter_transformation_fct(model,param_idx_loop)">parameter_transformation_fct</a>(model,idx);
0185     parameterRefTR(idx) = transfFunc.TR{i}(parameterRef(idx));
0186 <span class="keyword">end</span>
0187 parameterSearchInit   = parameterRef(parameter_search_idx);
0188 parameterSearchInitTR = parameterRefTR(parameter_search_idx);
0189 parameter             = parameterRef;
0190 parameterTR           = parameterRefTR;
0191 model.parameterTR     = parameterTR;
0192 <span class="comment">%% Parameter name</span>
0193 name_idx_1=<span class="string">''</span>;
0194 name_idx_2=<span class="string">''</span>;
0195 <span class="keyword">for</span> i=parameter_search_idx'
0196     name_p1{i}=[model.param_properties{i,1}];
0197     <span class="keyword">if</span> ~isempty(model.param_properties{i,4})
0198         temp=model.param_properties{i,4}(1);
0199     <span class="keyword">else</span>
0200         temp=<span class="string">''</span>;
0201     <span class="keyword">end</span>
0202     name_p2{i}=[model.param_properties{i,2}, <span class="string">'|M'</span>, model.param_properties{i,3},<span class="string">'|'</span>,temp];
0203     name_idx_1=[name_idx_1  name_p1{i} repmat(<span class="string">' '</span>,[1,15-length(name_p1{i})]) <span class="string">' '</span>];
0204     name_idx_2=[name_idx_2  name_p2{i} repmat(<span class="string">' '</span>,[1,15-length(name_p2{i})]) <span class="string">' '</span>];
0205 <span class="keyword">end</span>
0206 
0207 <span class="comment">%% Optimization process</span>
0208 logpdf              = zeros(1, maxEpochs);
0209 logpdfHist          = zeros(nb_param+1, maxEpochs * NminiBatch);
0210 idxMax_loop         = zeros(1, maxEpochs * NminiBatch);
0211 parameterSearch     = zeros(nb_param, maxEpochs);
0212 parameterSearchTR   = zeros(nb_param, maxEpochs);
0213 momentumTR          = zeros(nb_param, maxEpochs);
0214 RMSpropTR           = zeros(nb_param, maxEpochs);
0215 momentumTRhist      = zeros(nb_param, maxEpochs * NminiBatch);
0216 RMSpropTRhist       = zeros(nb_param, maxEpochs * NminiBatch);
0217 learningRate        = learningRateDefaut * ones(nb_param, maxEpochs * NminiBatch);
0218 gradientTR          = zeros(nb_param, maxEpochs * NminiBatch);
0219 hessianTR           = zeros(nb_param, maxEpochs * NminiBatch);
0220 zeroGradCount       = zeros(nb_param,1);
0221 paramMoveCount      = ones(nb_param,1);
0222 paramReset          = zeros(nb_param,1);
0223 metricVL            = zeros(1, maxEpochs);
0224 metricVLhist        = zeros(nb_param+1, maxEpochs * NminiBatch);
0225 mmtHessianTR        = zeros(nb_param, maxEpochs);
0226 mmtHessianTRhist    = zeros(nb_param, maxEpochs * NminiBatch);
0227 paramChange         = zeros(nb_param, maxEpochs);
0228 
0229 [metricVL(1),~, ~, logpdf(1)] = <a href="metricFct.html" class="code" title="function [metricVL, idxMaxM, logpdf_test, logpdf_train] = metricFct(data_train, data_test, model, misc, parameterSearch, parameterSearchTR)">metricFct</a>(data_train, data_valid, model, misc, parameter(parameter_search_idx), parameterTR(parameter_search_idx));
0230 parameterSearch(:,1)    = parameterSearchInit;
0231 parameterSearchTR(:,1)  = parameterSearchInitTR;
0232 gradientTR(:,1)         = NaN(nb_param, 1);
0233 hessianTR(:,1)          = NaN(nb_param, 1);
0234 
0235 tic; <span class="comment">% time counter initialization</span>
0236 Nepoch      = 1;
0237 Nloop       = 0;
0238 time_loop   = 0;
0239 stop_loop = 0;
0240 <span class="keyword">if</span> ~isMute
0241     fprintf(fileID, <span class="string">'\n'</span>);
0242     fprintf(fileID, <span class="string">'    Epoch #%s\n'</span>, num2str(Nepoch));
0243     fprintf(fileID, <span class="string">'             Metric: %s\n'</span>, num2str(metricVL(1)));
0244     fprintf(fileID, <span class="string">'                    %s\n'</span>, name_idx_2);
0245     fprintf(fileID, <span class="string">'   parameter names: %s\n'</span>, name_idx_1);
0246     fprintf(fileID, [<span class="string">'    initial values: '</span> repmat([<span class="string">'%#-+15.2e'</span> <span class="string">' '</span>],[1,length(parameterRef)]) <span class="string">'%#-+15.2e\n'</span>],parameterRef(parameter_search_idx));
0247     fprintf(fileID, <span class="string">'\n'</span>);
0248 <span class="keyword">end</span>
0249 <span class="keyword">while</span> Nepoch &lt;= maxEpochs &amp;&amp; time_loop &lt; maxTime*60
0250     Nepoch                  = Nepoch + 1;
0251     parameterSearch_loop    = parameter(parameter_search_idx);
0252     parameterSearchTR_loop  = parameterTR(parameter_search_idx);
0253     momentumTR_loop         = momentumTR(:,Nepoch - 1);
0254     RMSpropTR_loop          = RMSpropTR(:, Nepoch - 1);
0255     loopCounter1epoch       = 0;
0256     mmtHessianTR_loop       = mmtHessianTR(:, Nepoch-1);
0257     <span class="keyword">while</span> loopCounter1epoch &lt; NminiBatch
0258         Nloop       = Nloop + 1;
0259         loopCounter1epoch       = loopCounter1epoch + 1;
0260         <span class="comment">% Ramdomly chose the start point for mini batch</span>
0261         datapointbound          = length(data_train.values) - Ndata4miniBatch;
0262         rng(<span class="string">'shuffle'</span>)
0263         idxDStart               = randi([1, datapointbound], 1);
0264         idxData4miniBatch       = idxDStart:idxDStart+Ndata4miniBatch;
0265         [data_miniBatch, data_miniBatchTest]    = <a href="#_sub1" class="code" title="subfunction [data_train, data_valid] = dataSplit(data, idxTrain, alpha_split, varargin)">dataSplit</a>(data_train, idxData4miniBatch , alpha_split,<span class="string">'getTimeStepRef'</span>,0);
0266         <span class="comment">% Parameter setup</span>
0267         model.parameter(parameter_search_idx)   = parameterSearch_loop;
0268         model.parameterTR(parameter_search_idx) = parameterSearchTR_loop;
0269         delta_grad_loop                         = delta_grad(parameter_search_idx);
0270         param_idx_loop                          = parameter_search_idx;
0271         parfor p = 1:nb_param
0272             <span class="comment">% First and second derivatives computations</span>
0273             [~, GlogpdfTR_loop, HlogpdfTR_loop, ~] = <a href="logPosteriorPE.html" class="code" title="function [logpdf, Glogpdf, Hlogpdf, delta_grad] = logPosteriorPE(data, model, misc, varargin)">logPosteriorPE</a>(data_miniBatch, model, misc,<span class="keyword">...</span>
0274                 <span class="string">'paramTR_index'</span>,param_idx_loop(p),<span class="keyword">...</span>
0275                 <span class="string">'stepSize4grad'</span>,delta_grad_loop(p));
0276             <span class="comment">% Store the 1st and 2nd derivatives</span>
0277             <span class="keyword">if</span> isnan(GlogpdfTR_loop)
0278                 gradientTR(p, Nloop) = 0;
0279             <span class="keyword">else</span>
0280                 gradientTR(p, Nloop)  = GlogpdfTR_loop;
0281             <span class="keyword">end</span>
0282             hessianTR(p, Nloop)     = HlogpdfTR_loop;
0283             <span class="comment">% Learning rate</span>
0284             <span class="keyword">if</span> strcmp(learningRate_mode,<span class="string">'hessian'</span>)
0285                 <span class="keyword">if</span> HlogpdfTR_loop &lt; 0
0286                     learningRate(p, Nloop)  = -1/HlogpdfTR_loop;
0287                 <span class="keyword">elseif</span> HlogpdfTR_loop &gt; 0
0288                     learningRate(p, Nloop)  = 1/HlogpdfTR_loop;
0289                 <span class="keyword">elseif</span> isnan(HlogpdfTR_loop)
0290                     learningRate(p, Nloop)=learningRateDefaut/(sqrt(paramMoveCount(p)));
0291                 <span class="keyword">end</span>
0292             <span class="keyword">elseif</span> strcmp(learningRate_mode,<span class="string">'decay'</span>)
0293                 learningRate(p, Nloop)=learningRateDefaut/(sqrt(paramMoveCount(p)));
0294             <span class="keyword">elseif</span> strcmp(learningRate_mode,<span class="string">'defaut'</span>)
0295                 learningRate(p, Nloop) = learningRateDefaut;
0296             <span class="keyword">end</span>
0297         <span class="keyword">end</span>
0298         <span class="comment">% Learning rate constrains to avoid the exploding gradient problem.</span>
0299         <span class="comment">% See Recurent Neural Network (RNN) for futher details.</span>
0300         <span class="keyword">if</span> any(any(isnan(hessianTR(:,Nloop))))
0301             hessianTR(:,Nloop)=hessianDefaut;
0302         <span class="keyword">end</span>
0303         idx_lr = abs(hessianTR(:,Nloop))&lt;0.001;
0304         <span class="keyword">if</span> any(any(idx_lr))
0305             <span class="keyword">if</span> Nloop==1
0306                 hessianTR(idx_lr,Nloop)= hessianDefaut ;
0307             <span class="keyword">else</span>
0308                 hessianTR(idx_lr,Nloop)= hessianDefaut.*(sqrt(paramMoveCount(idx_lr)));
0309             <span class="keyword">end</span>
0310         <span class="keyword">end</span>
0311         idx_lr = abs(learningRate(:,Nloop))&gt;1000;
0312         <span class="keyword">if</span> any(any(idx_lr))
0313             <span class="keyword">if</span> Nloop==1
0314                 learningRate(idx_lr,Nloop)= learningRateDefaut;
0315             <span class="keyword">else</span>
0316                 learningRate(idx_lr,Nloop)= learningRate(idx_lr,Nloop)./sqrt(paramMoveCount(idx_lr));
0317             <span class="keyword">end</span>
0318         <span class="keyword">end</span>
0319         
0320         <span class="comment">% Parameter initialization in order to avoid the vanishing gradient</span>
0321         <span class="comment">% problem. See Recurent Neural Network (RNN) for further details.</span>
0322         <span class="keyword">if</span> any(abs(gradientTR(:, Nloop))&lt;1E-2)
0323             idxZeroGrad                     = abs(gradientTR(:, Nloop))&lt;1E-2;
0324             zeroGradCount(idxZeroGrad)      = zeroGradCount(idxZeroGrad)+1;
0325             zeroGradCount(~idxZeroGrad)     = 0;
0326             idxG                            = zeroGradCount&gt;0;
0327             momentumTR_loop(idxG)           = 0;
0328             RMSpropTR_loop(idxG)            = 0;
0329             gradientTR(idxG, Nloop)         = 0;
0330             paramMoveCount(idxG)            = 1;
0331             paramReset(idxG)                = paramReset(idxG) + 1;
0332             parameterRandom                 = mvnrnd(parameterSearchTR(idxG,1),stdInit*diag(ones(size(parameterSearchTR(idxG,1),1),1)));
0333             parameterSearchTR_loop(idxG)    = parameterRandom';
0334             <span class="keyword">for</span> p = 1:nb_param
0335                 parameterSearch_loop(p) = transfFunc.InvTR{p}(parameterSearchTR_loop(p));
0336             <span class="keyword">end</span>
0337             mmtHessianTR_loop(idxG) = 0;
0338         <span class="keyword">end</span>
0339         <span class="comment">% Grid search setup</span>
0340         [parameterSearchTRold_loop,<span class="keyword">...</span>
0341             momentumTRold_loop,<span class="keyword">...</span>
0342             RMSpropTRold_loop,<span class="keyword">...</span>
0343             gradientTRold_loop,<span class="keyword">...</span>
0344             learningRateTRold_loop,<span class="keyword">...</span>
0345             mmtHessianTRold_loop,<span class="keyword">...</span>
0346             hessianTRold_loop]         = <a href="#_sub6" class="code" title="subfunction [xM, momentumM, RMSpropM, gradM, learningRateM, mmtHessM, hessM]= paramGrid(x, momentum, RMSprop, grad, learningRate, mmtHess, hess)">paramGrid</a>(parameterSearchTR_loop, momentumTR_loop, RMSpropTR_loop, gradientTR(:, Nloop), learningRate(:, Nloop), mmtHessianTR_loop, hessianTR(:, Nloop));
0347         parameterSearchNew_loop     = zeros(nb_param, nb_param+1);
0348         parameterSearchTRNew_loop   = zeros(nb_param, nb_param+1);
0349         momentumTRNew_loop          = zeros(nb_param, nb_param+1);
0350         RMSpropTRNew_loop           = zeros(nb_param, nb_param+1);
0351         funcInvTR                   = transfFunc.InvTR;
0352         mmtHessianTRNew_loop        = zeros(nb_param, nb_param+1);
0353         
0354         <span class="comment">% New parameter values computed using ADAM or MMT</span>
0355         <span class="keyword">if</span> strcmp(optimizer,<span class="string">'ADAM'</span>)
0356             parfor p =1:nb_param+1
0357                 [parameterSearchNew_loop(:,p),<span class="keyword">...</span>
0358                     parameterSearchTRNew_loop(:,p),<span class="keyword">...</span>
0359                     momentumTRNew_loop(:,p),<span class="keyword">...</span>
0360                     RMSpropTRNew_loop(:,p)] = <a href="#_sub4" class="code" title="subfunction [xsearch, xsearchTR, momentumTR, RMSpropTR] = ADAM(xsearchTRprev, momentumTRprev, RMSpropTRprev, grad, step, beta_1, beta_2, epsilon, Niter, fctInvTR)">ADAM</a>(parameterSearchTRold_loop(:,p), momentumTRold_loop(:,p), RMSpropTRold_loop(:,p), gradientTRold_loop(:,p), learningRateTRold_loop(:, p), beta_1, beta_2, epsilon, paramMoveCount, funcInvTR);
0361             <span class="keyword">end</span>
0362         <span class="keyword">elseif</span> strcmp(optimizer,<span class="string">'MMT'</span>)
0363             parfor p = 1:nb_param+1
0364                 [parameterSearchNew_loop(:,p),<span class="keyword">...</span>
0365                     parameterSearchTRNew_loop(:,p),<span class="keyword">...</span>
0366                     momentumTRNew_loop(:,p)] = <a href="#_sub2" class="code" title="subfunction [xsearch, xsearchTR, momentumTR] = MMT(xsearchTRprev, momentumTRprev, grad, step, beta, fctInvTR)">MMT</a>(parameterSearchTRold_loop(:,p), momentumTRold_loop(:,p), gradientTRold_loop(:,p), learningRateTRold_loop(:, p), beta_1, funcInvTR );
0367             <span class="keyword">end</span>
0368         <span class="keyword">elseif</span> strcmp(optimizer,<span class="string">'ADAMbeta'</span>)
0369             parfor p = 1:nb_param+1
0370                 [parameterSearchNew_loop(:,p),<span class="keyword">...</span>
0371                     parameterSearchTRNew_loop(:,p),<span class="keyword">...</span>
0372                     momentumTRNew_loop(:,p),<span class="keyword">...</span>
0373                     RMSpropTRNew_loop(:,p),<span class="keyword">...</span>
0374                     mmtHessianTRNew_loop(:,p)] = <a href="#_sub5" class="code" title="subfunction [xsearch, xsearchTR, momentumTR, RMSpropTR, mmtHessianTR] = ADAMbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, RMSpropTRprev, grad, hess, beta_1, beta_2, epsilon, Niter, fctInvTR)">ADAMbeta</a>(parameterSearchTRold_loop(:,p), momentumTRold_loop(:,p), mmtHessianTRold_loop(:,p), RMSpropTRold_loop(:,p), gradientTRold_loop(:,p), hessianTRold_loop(:,p), beta_1, beta_2, epsilon, paramMoveCount, funcInvTR);
0375             <span class="keyword">end</span>
0376         <span class="keyword">elseif</span> strcmp(optimizer,<span class="string">'MMTbeta'</span>)
0377             parfor p = 1:nb_param+1
0378                 [parameterSearchNew_loop(:,p),<span class="keyword">...</span>
0379                     parameterSearchTRNew_loop(:,p),<span class="keyword">...</span>
0380                     momentumTRNew_loop(:,p),<span class="keyword">...</span>
0381                     mmtHessianTRNew_loop(:,p)] = <a href="#_sub3" class="code" title="subfunction [xsearch, xsearchTR, momentumTR, mmtHessianTR] = MMTbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, grad, hess, beta, fctInvTR)">MMTbeta</a>(parameterSearchTRold_loop(:,p), momentumTRold_loop(:,p), mmtHessianTRold_loop(:,p), gradientTRold_loop(:,p), hessianTRold_loop(:,p), beta_1, funcInvTR );
0382             <span class="keyword">end</span>
0383         <span class="keyword">end</span>
0384         <span class="comment">% Parameter selection for the mini batch</span>
0385         [metricVLhist(:,Nloop),idxMaxPC, ~, logpdfHist(:,Nloop)] = <a href="metricFct.html" class="code" title="function [metricVL, idxMaxM, logpdf_test, logpdf_train] = metricFct(data_train, data_test, model, misc, parameterSearch, parameterSearchTR)">metricFct</a>(data_miniBatch, data_miniBatchTest, model, misc, parameterSearchNew_loop, parameterSearchTRNew_loop);
0386         <span class="comment">%if strcmp(misc.metric_mode,'predCap')</span>
0387         <span class="keyword">if</span> isPredCap
0388             [idxMaxlogpdf,~]=nanmax(logpdfHist(:,Nloop));
0389             <span class="keyword">if</span> idxMaxlogpdf==idxMaxPC
0390                 idxMax_loop(Nloop)=1;
0391             <span class="keyword">else</span>
0392                 idxMax_loop(Nloop) = 0;
0393             <span class="keyword">end</span>
0394         <span class="keyword">else</span>
0395             idxMax_loop(Nloop) = NaN;
0396         <span class="keyword">end</span>
0397         <span class="comment">% Parameter update for the mini batch</span>
0398         momentumTR_loop  = momentumTRNew_loop(:,end);
0399         RMSpropTR_loop   = RMSpropTRNew_loop(:,end);
0400         mmtHessianTR_loop = mmtHessianTRNew_loop(:,end);
0401         <span class="keyword">if</span> idxMaxPC ~= nb_param+1
0402             idxMomentum                  = momentumTRNew_loop(:,idxMaxPC)~=0;
0403             momentumTR_loop(:)           = 0;
0404             RMSpropTR_loop(:)            = 0;
0405             momentumTR_loop(idxMomentum) = momentumTRNew_loop(idxMomentum,idxMaxPC);
0406             RMSpropTR_loop(idxMomentum)  = RMSpropTRNew_loop(idxMomentum,idxMaxPC);
0407             paramMoveCount(idxMomentum)  = paramMoveCount(idxMomentum)+1;
0408             mmtHessianTR_loop(idxMomentum)= mmtHessianTRNew_loop(idxMomentum,idxMaxPC);
0409         <span class="keyword">else</span>
0410             paramMoveCount = paramMoveCount+1;
0411         <span class="keyword">end</span>
0412         parameterSearchTR_loop  = parameterSearchTRNew_loop(:,idxMaxPC);
0413         parameterSearch_loop    = parameterSearchNew_loop(:,idxMaxPC);
0414         momentumTRhist(:,Nloop) = momentumTR_loop;
0415         RMSpropTRhist(:,Nloop)  = RMSpropTR_loop;
0416         mmtHessianTRhist(:,Nloop)= mmtHessianTR_loop;
0417     <span class="keyword">end</span>
0418     <span class="comment">% Metric calculation for each epoch</span>
0419     [metricVL(Nepoch),~, ~, logpdf(Nepoch)] = <a href="metricFct.html" class="code" title="function [metricVL, idxMaxM, logpdf_test, logpdf_train] = metricFct(data_train, data_test, model, misc, parameterSearch, parameterSearchTR)">metricFct</a>(data_train, data_valid, model, misc, parameterSearch_loop, parameterSearchTR_loop);
0420     
0421     <span class="keyword">if</span> sign(metricVL(Nepoch-1))==-1
0422         TL_loop = termination_tolerance+1;
0423     <span class="keyword">else</span>
0424         TL_loop = termination_tolerance;
0425     <span class="keyword">end</span>
0426     
0427     <span class="comment">% Parameter update for the training set</span>
0428     <span class="keyword">if</span> or(metricVL(Nepoch) &gt; metricVL(Nepoch-1)*TL_loop,logpdf(Nepoch) &gt; logpdf(Nepoch-1)*TL_loop)
0429         parameterSearch(:,Nepoch)           = parameterSearch_loop;
0430         parameterSearchTR(:,Nepoch)         = parameterSearchTR_loop;
0431         parameter(parameter_search_idx)     = parameterSearch(:,Nepoch);
0432         parameterTR(parameter_search_idx)   = parameterSearchTR(:,Nepoch);
0433         momentumTR(:,Nepoch)                = momentumTR_loop;
0434         RMSpropTR(:,Nepoch)                 = RMSpropTR_loop;
0435         stop_loop = 0;
0436         mmtHessianTR(:,Nepoch)              = mmtHessianTR_loop;
0437     <span class="keyword">else</span>
0438         stop_loop = stop_loop + 1;
0439         <span class="keyword">if</span> stop_loop &gt; 3
0440             stop_loop=0;
0441             [metricVL(Nepoch),idxMaxEpochs]   = nanmax(metricVL);
0442             parameterSearch(:,Nepoch)         = parameterSearch(:,idxMaxEpochs);
0443             parameterSearchTR(:,Nepoch)       = parameterSearchTR(:,idxMaxEpochs);
0444             parameter(parameter_search_idx)   = parameterSearch(:,idxMaxEpochs);
0445             parameterTR(parameter_search_idx) = parameterSearchTR(:,idxMaxEpochs);
0446             momentumTR(:,Nepoch)              = momentumTR(:,idxMaxEpochs);
0447             RMSpropTR(:,Nepoch)               = RMSpropTR(:,idxMaxEpochs);
0448             logpdf(Nepoch)                    = logpdf(idxMaxEpochs);
0449             metricVL(Nepoch)                  = metricVL(idxMaxEpochs);
0450             momentumTRhist(:,Nloop)           = momentumTR(:,Nepoch);
0451             RMSpropTRhist(:,Nloop)            = RMSpropTR(:,Nepoch);
0452             mmtHessianTR(:, Nepoch)           = mmtHessianTR(:,idxMaxEpochs);
0453         <span class="keyword">else</span>
0454             parameterSearch(:,Nepoch)           = parameterSearch(:,Nepoch-1);
0455             parameterSearchTR(:,Nepoch)         = parameterSearchTR(:,Nepoch-1);
0456             parameter(parameter_search_idx)     = parameterSearch(:,Nepoch-1);
0457             parameterTR(parameter_search_idx)   = parameterSearchTR(:,Nepoch-1);
0458             momentumTR(:,Nepoch)                = momentumTR(:,1);
0459             RMSpropTR(:,Nepoch)                 = RMSpropTR(:,1);
0460             logpdf(Nepoch)                      = logpdf(Nepoch-1);
0461             metricVL(Nepoch)                    = metricVL(Nepoch-1);
0462             momentumTRhist(:,Nloop)             = momentumTR(:,Nepoch);
0463             RMSpropTRhist(:,Nloop)              = RMSpropTR(:,Nepoch);
0464             mmtHessianTR(:, Nepoch)             = mmtHessianTR(:,1);
0465         <span class="keyword">end</span>
0466     <span class="keyword">end</span>
0467     paramChange(:,Nepoch)  = (parameterSearch(:,Nepoch)-parameterSearch(:,Nepoch-1));
0468     
0469     <span class="keyword">if</span> ~isMute
0470         fprintf(fileID, <span class="string">'\n'</span>);
0471         fprintf(fileID, <span class="string">'--------------------------\n'</span>);
0472         fprintf(fileID, <span class="string">'    Epoch #%s\n'</span>, num2str(Nepoch));
0473         fprintf(fileID, <span class="string">'            Metric: %s\n'</span>, num2str(metricVL(Nepoch)));
0474         fprintf(fileID, <span class="string">'   parameter names: %s\n'</span>, name_idx_2);
0475         fprintf(fileID, [<span class="string">'    current values: '</span> repmat([<span class="string">'%#-+15.2e'</span> <span class="string">' '</span>],[1,length(parameter(parameter_search_idx))-1]) <span class="string">'%#-+15.2e\n'</span>,<span class="keyword">...</span>
0476             <span class="string">'      param change: '</span> repmat([<span class="string">'%#-+15.2e'</span> <span class="string">' '</span>],[1,length(parameter(parameter_search_idx))-1]) <span class="string">'%#-+15.2e\n'</span>,<span class="keyword">...</span>
0477             <span class="string">'  initialize param: '</span> repmat([<span class="string">'%#-+15.2e'</span> <span class="string">' '</span>],[1,length(parameter(parameter_search_idx))-1]) <span class="string">'%#-+15.2e\n'</span>],<span class="keyword">...</span>
0478             parameter(parameter_search_idx),<span class="keyword">...</span>
0479             paramChange(:,Nepoch),<span class="keyword">...</span>
0480             paramReset);
0481         fprintf(fileID, <span class="string">'\n'</span>);
0482     <span class="keyword">end</span>
0483     time_loop=toc;
0484 <span class="keyword">end</span>
0485 
0486 <span class="comment">%% Selection of the optimal parameters corresponding to the best log-post</span>
0487 [metricVLmax,idxmax]                    = nanmax(metricVL(1:Nepoch));
0488 parameterSearch_opt                     = parameterSearch(:,idxmax);
0489 parameterSearchTR_opt                   = parameterSearchTR(:,idxmax);
0490 parameter_opt                           = parameter;
0491 parameter_opt(parameter_search_idx)     = parameterSearch_opt;
0492 parameterTR_opt                         = parameterTR;
0493 parameterTR_opt(parameter_search_idx)   = parameterSearchTR_opt;
0494 
0495 <span class="comment">%% Output</span>
0496 optim.parameter_opt         = parameter_opt;
0497 optim.parameterTR_opt       = parameterTR_opt;
0498 optim.metricVLmax           = metricVLmax;
0499 optim.metricVL              = metricVL;
0500 optim.logpdf                = logpdf;
0501 optim.log_lik               = nanmax(logpdf);
0502 optim.Nepoch                = Nepoch;
0503 optim.converged             = 0;
0504 optim.gradientTR            = gradientTR;
0505 optim.learningRate          = learningRate;
0506 optim.hessianTR             = hessianTR;
0507 optim.parameterSearch       = parameterSearch;
0508 optim.parameterSearchTR     = parameterSearchTR;
0509 optim.momentumTR            = momentumTR;
0510 optim.RMSpropTR             = RMSpropTR;
0511 optim.Ndata4miniBatch       = Ndata4miniBatch;
0512 optim.idxMax_loop           = idxMax_loop;
0513 optim.optim_mode            = optim_mode;
0514 optim.beta_1                = beta_1;
0515 optim.beta_2                = beta_2;
0516 optim.epsilon               = epsilon;
0517 optim.learningRateDefaut    = learningRateDefaut;
0518 optim.data_train            = data_train;
0519 optim.data_valid            = data_valid;
0520 optim.data                  = data;
0521 optim.data_train            = data_valid;
0522 optim.misc                  = misc;
0523 
0524 <span class="comment">%% Write model.parameters in model.param_properties</span>
0525 
0526 <span class="comment">% Add parameter and p_ref to param_properties</span>
0527 parameter=optim.parameter_opt;
0528 
0529 [model.param_properties]=<a href="../../../OpenBDLM_V1.0/functions/Others/writeParameterProperties.html" class="code" title="function [cellOut]=writeParameterProperties(cellIn, arrayIn, Position)">writeParameterProperties</a>(model.param_properties, <span class="keyword">...</span>
0530     [parameter, p_ref], 9);
0531 
0532 <span class="keyword">end</span>
0533 
0534 <a name="_sub1" href="#_subfunctions" class="code">function [data_train, data_valid] = dataSplit(data, idxTrain, alpha_split, varargin)</a>
0535 getTimeStepRef = 1;
0536 timeStepMean   = 0;
0537 args    = varargin;
0538 nargs   = length(varargin);
0539 tol     = 1e-4;
0540 <span class="keyword">for</span> n = 1:2:nargs
0541     <span class="keyword">switch</span> args{n}
0542         <span class="keyword">case</span> <span class="string">'getTimeStepRef'</span>,   getTimeStepRef = args{n+1};
0543         <span class="keyword">case</span> <span class="string">'timeStepMean'</span>,     timeStepMean   = args{n+1};
0544         <span class="keyword">otherwise</span>, error([<span class="string">'Unrecognized argument'</span> args{n}])
0545     <span class="keyword">end</span>
0546 <span class="keyword">end</span>
0547 dataTrainingProcess.values      = data.values(idxTrain,:);
0548 dataTrainingProcess.timestamps  = data.timestamps(idxTrain,:);
0549 NdataTrainingProcess            = size(dataTrainingProcess.values ,1);
0550 NdataValid                      = round(alpha_split*NdataTrainingProcess);
0551 
0552 data_train.values       = dataTrainingProcess.values(1:NdataTrainingProcess-NdataValid,:);
0553 data_train.timestamps   = dataTrainingProcess.timestamps(1:NdataTrainingProcess-NdataValid,:);
0554 data_train.nb_steps     = size(data_train.values,1);
0555 
0556 data_valid              = dataTrainingProcess;
0557 data_valid.nb_steps     = size(data_valid.values,1);
0558 <span class="keyword">if</span> getTimeStepRef==1
0559     data_train.dt_steps         = diff(data_train.timestamps);
0560     <span class="keyword">if</span> timeStepMean==1
0561         data_train.dt_ref = mean(data_train.dt_steps);
0562     <span class="keyword">else</span>
0563         unique_dt_steps             = uniquetol(data_train.dt_steps,tol);
0564         counts_dt_steps             = [unique_dt_steps,histc(data_train.dt_steps(:),unique_dt_steps)];
0565         data_train.dt_ref           = counts_dt_steps(find(counts_dt_steps(:,2)==max(counts_dt_steps(:,2)),1,<span class="string">'first'</span>),1);
0566     <span class="keyword">end</span>
0567     data_train.dt_steps = [data_train.dt_ref;data_train.dt_steps];
0568     data_valid.dt_steps = diff(data_valid.timestamps);
0569     data_valid.dt_ref   = data_train.dt_ref;
0570     data_valid.dt_steps = [data_valid.dt_ref; data_valid.dt_steps];
0571 <span class="keyword">else</span>
0572     data_train.dt_ref   = data.dt_ref;
0573     data_train.dt_steps = data.dt_steps(1:NdataTrainingProcess-NdataValid,:);
0574     data_valid.dt_ref   = data.dt_ref;
0575     data_valid.dt_steps = data.dt_steps(1:NdataTrainingProcess,:);
0576 <span class="keyword">end</span>
0577 <span class="keyword">end</span>
0578 
0579 <a name="_sub2" href="#_subfunctions" class="code">function [xsearch, xsearchTR, momentumTR] = MMT(xsearchTRprev, momentumTRprev, grad, step, beta, fctInvTR)</a>
0580 xsearch     = zeros(length(xsearchTRprev), 1);
0581 momentumTR   = beta*momentumTRprev + (1-beta)*grad;
0582 xsearchTR   = xsearchTRprev + step.*momentumTR;
0583 <span class="keyword">for</span> p = 1:length(xsearchTRprev)
0584     xsearch(p) =  fctInvTR{p}(xsearchTR(p));
0585 <span class="keyword">end</span>
0586 <span class="keyword">end</span>
0587 
0588 <a name="_sub3" href="#_subfunctions" class="code">function [xsearch, xsearchTR, momentumTR, mmtHessianTR] = MMTbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, grad, hess, beta, fctInvTR)</a>
0589 xsearch      = zeros(length(xsearchTRprev), 1);
0590 momentumTR   = beta*momentumTRprev + (1-beta)*grad;
0591 mmtHessianTR = beta*mmtHessianTRprev + (1-beta)*abs(hess);
0592 step         = 1./mmtHessianTR;
0593 step(step==Inf)= 0;
0594 xsearchTR    = xsearchTRprev + step.*momentumTR;
0595 <span class="keyword">for</span> p = 1:length(xsearchTRprev)
0596     xsearch(p) =  fctInvTR{p}(xsearchTR(p));
0597 <span class="keyword">end</span>
0598 <span class="keyword">end</span>
0599 
0600 <a name="_sub4" href="#_subfunctions" class="code">function [xsearch, xsearchTR, momentumTR, RMSpropTR] = ADAM(xsearchTRprev, momentumTRprev, RMSpropTRprev, grad, step, beta_1, beta_2, epsilon, Niter, fctInvTR)</a>
0601 xsearch         = zeros(length(xsearchTRprev), 1);
0602 momentumTR      = beta_1*momentumTRprev + (1-beta_1)*grad;
0603 RMSpropTR       = beta_2*RMSpropTRprev + (1-beta_2)*grad.^2;
0604 momentumTRcorr  = momentumTR./(1-beta_1.^Niter);
0605 RMSpropTRcorr   = RMSpropTR./(1-beta_2.^Niter);
0606 xsearchTR       = xsearchTRprev + step.*momentumTRcorr./(sqrt(RMSpropTRcorr)+epsilon);
0607 <span class="keyword">for</span> p = 1:length(xsearchTRprev)
0608     xsearch(p) =  fctInvTR{p}(xsearchTR(p));
0609 <span class="keyword">end</span>
0610 <span class="keyword">end</span>
0611 
0612 <a name="_sub5" href="#_subfunctions" class="code">function [xsearch, xsearchTR, momentumTR, RMSpropTR, mmtHessianTR] = ADAMbeta(xsearchTRprev, momentumTRprev, mmtHessianTRprev, RMSpropTRprev, grad, hess, beta_1, beta_2, epsilon, Niter, fctInvTR)</a>
0613 xsearch         = zeros(length(xsearchTRprev), 1);
0614 momentumTR      = beta_1*momentumTRprev + (1-beta_1)*grad;
0615 mmtHessianTR    = beta_1*mmtHessianTRprev + (1-beta_1)*abs(hess);
0616 RMSpropTR       = beta_2*RMSpropTRprev + (1-beta_2)*grad.^2;
0617 momentumTRcorr  = momentumTR./(1-beta_1.^Niter);
0618 mmtHessTRcorr   = mmtHessianTR./(1-beta_1.^Niter);
0619 RMSpropTRcorr   = RMSpropTR./(1-beta_2.^Niter);
0620 step            = 1./mmtHessTRcorr;
0621 step(step==Inf) = 0;
0622 xsearchTR       = xsearchTRprev + step.*momentumTRcorr./(sqrt(RMSpropTRcorr)+epsilon);
0623 <span class="keyword">for</span> p = 1:length(xsearchTRprev)
0624     xsearch(p) =  fctInvTR{p}(xsearchTR(p));
0625 <span class="keyword">end</span>
0626 <span class="keyword">end</span>
0627 
0628 <a name="_sub6" href="#_subfunctions" class="code">function [xM, momentumM, RMSpropM, gradM, learningRateM, mmtHessM, hessM]= paramGrid(x, momentum, RMSprop, grad, learningRate, mmtHess, hess)</a>
0629 d             = length(x);
0630 xM            = repmat(x,1,d+1);
0631 momentumM     = [diag(momentum) momentum];
0632 mmtHessM      = [diag(mmtHess) mmtHess];
0633 RMSpropM      = [diag(RMSprop) RMSprop];
0634 gradM         = [diag(grad) grad];
0635 hessM         = [diag(hess) hess];
0636 learningRateM = [diag(learningRate) learningRate];
0637 <span class="keyword">end</span></pre></div>
<hr><address>Generated on Tue 05-Feb-2019 11:23:52 by <strong><a href="http://www.artefact.tk/software/matlab/m2html/" title="Matlab Documentation in HTML">m2html</a></strong> &copy; 2005</address>
</body>
</html>