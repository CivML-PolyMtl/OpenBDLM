\section{Reference Theory}
This section presents a summary of the theory behind Bayesian Dynamic Linear Models. For in-depth details, the reader should consult the following references:\\[4pt]

\noindent \emph{A Kernel-based Method for Modeling Non-Harmonic Periodic Phenomena in Bayesian Dynamic Linear Models}\\{\small
            Nguyen, L.H., Gaudot, I., Shervin Khazaeli and Goulet, J.-A.\\
            Frontiers in Built Environment. Vol. X, Issue X, pp, 2019\\}
      [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Nguyen_et_al_KR_BDLM_2019.pdf}{PDF}] [~]  [~] [~] \cite{Nguyen2019KRBDLM}\\[4pt]

\noindent \emph{Uncertainty quantification for model parameters and hidden state variables in bayesian dynamic linear models}\\{\small
            Nguyen, L.H., Gaudot, I., and Goulet, J.-A.\\
            Structural Control and Health Monitoring. Vol. X, Issue X, pp.e2136, 2018\\}
      [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Nguyen_Gaudot_Goulet_MCMC_BDLM_2018.pdf}{PDF}] [~]  [~] [\href{https://doi.org/10.1002/stc.2309}{DOI link}] \cite{Nguyen2018UncertaintyBDLM}\\[4pt]
      
      \noindent \emph{Anomaly Detection with the Switching Kalman Filter for Structural Health Monitoring}\\{\small
            Nguyen, L.H. and Goulet, J.-A.\\
            Structural Control and Health Monitoring. Vol. 24, Issue 4, pp.e2136, 2018\\}
      [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/2017_Nguyen_and_Goulet_AD-SKF.pdf}{PDF}] [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Nguyen_SKF_2018.xml}{Endnote}]  [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Nguyen_SKF_2018.ris}{BibTeX}] [\href{https://doi.org/10.1002/stc.2136}{DOI link}] \cite{Nguyen2018}\\[4pt]

\noindent \emph{Structural health monitoring with dependence on non-harmonic periodic hidden covariates}\\{\small
            Nguyen, L.H. and Goulet, J.-A.\\
            Engineering Structures, 166:187 √ê 194., 2018\\}
      [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/2018_Nguyen_et_Goulet_SHMHNHC.pdf}{PDF}] [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/2018_Nguyen_et_Goulet_HNHC.xml}{Endnote}]  [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/2018_Nguyen_et_Goulet_HNHC.bib}{BibTeX}] [\href{https://doi.org/10.1016/j.engstruct.2018.03.080}{DOI link}] \cite{Nguyen2018187}\\[4pt]

\noindent \emph{Empirical validation of Bayesian Dynamic Linear Models in the context of Structural Health Monitoring}\\{\small
            Goulet, J.-A. and Koo, K.\\
            Journal of Bridge Engineering. Vol. 23, Issue 2, pp. 05017017, 2018\\}
      [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Goulet_BDLM_tamar_2017.pdf}{PDF}] [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Goulet_BDLM_2018.xml}{Endnote}]  [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Goulet_BDLM_2018.ris}{BibTeX}] [\href{https://doi.org/10.1061/\%28ASCE\%29BE.1943-5592.0001190}{DOI link}] \cite{Goulet2017BDLMEmprical}\\[4pt]

\noindent \emph{Bayesian dynamic linear models for structural health monitoring}\\{\small
            Goulet, J.-A.\\
            Structural Control and Health Monitoring. Vol. 24, Issue 12, pp.e2025, 2017\\}
      [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Goulet_BDLM_SHM_2017_preprint.pdf}{PDF}] [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Goulet_BDLM_2017.xml}{Endnote}]  [\href{https://www.polymtl.ca/cgm/jagoulet/Site/Papers/Goulet_BDLM_2017.ris}{BibTeX}] [\href{https://doi.org/10.1002/stc.2035}{DOI link}] \cite{STC:STC2035} \\

 
\subsection{Linear gaussian state-space model}
\label{SS:LGSSM}
OpenBDLM builds on Bayesian dynamic linear models (BDLMs).
Bayesian dynamic linear models \cite{west1999bayesian} are a class of linear gaussian state-space models which can be described from the transition and the observation equations.
The transition equation describes the dynamics of the system, and is formulated as
\begin{equation}
  \mathbf{x}_{t}=\mathbf{A}_{t}\mathbf{x}_{t-1}+\mathbf{w}_{t},\quad\left\{
  \begin{array}{l}
\mathbf{x}_{t}\sim \mathcal{N}(\bm{\mu}_{t},\bm{\Sigma}_{t})\\[4pt]
\mathbf{w}_{t}\sim \mathcal{N}(\mathbf{0},
\mathbf{Q}_{t}),
\end{array}\right.
\label{EQ:SSM_Transition}
\end{equation}
where, for each each time $t=1, \dots ,\mathtt{T}$, the variables $\mathbf{x}_{t}$ follow a Gaussian distribution with mean $\bm{\mu}_{t}$ and covariance matrix $\bm{\Sigma}_{t}$, $\mathbf{A}_{t}$ is the transition matrix, and $\mathbf{w}_{t}$ represents Gaussian model errors with zero mean and covariance matrix $\mathbf{Q}_{t}$.
The variables $\mathbf{x}_{t}$ are usually referred to as hidden states because they are not directly observed.
The relationship between the observations $\mathbf{y}_{t}$ and the hidden states $\mathbf{x}_{t}$ is given by the observation equation, such as
\begin{equation}
\mathbf{y}_{t}=\mathbf{C}_{t}\mathbf{x}_{t}+\mathbf{v}_{t},\quad\left\{\begin{array}{l}
\mathbf{v}_{t}\sim \mathcal{N}(\mathbf{0},\mathbf{R}_{t}),
\end{array}\right.
\label{EQ:SSM_Observation}
\end{equation}
where $\mathbf{C}_{t}$ is the observation matrix, and $\mathbf{v}_{t}$ is the Gaussian measurement error with zero mean and covariance matrix $\mathbf{R}_{t}$.
BDLMs are capable of analyzing multiple time series simultaneously.
In case of dependencies between the time series, regression coefficients are added in $\mathbf{C}_{t}$ (see Section~\ref{S:Dependencies} and \cite{STC:STC2035}).
One particularity of BDLMs is their capacity to update the current estimated state with the current observations, thus allowing to perform online state inference of non-stationary time series.

\subsection{Kalman filter \& UD filters}
\label{SS:KFUD}
The analytical solutions for the prediction, observation and update step are available through either the Kalman filter (KF) or the UD filter, which can be expressed in its short form as:
\begin{equation}
    \begin{split}
      (\bm{\mu}_{t|t},\bm{\Sigma}_{t|t}, \mathcal{L}_{t}) = \text{Filter}(\bm{\mu}_{t-1|t-1},\bm{\Sigma}_{t-1|t-1},\mathbf{y}_{t}, \mathbf{A}_{t},  \mathbf{Q}_{t},   \mathbf{C}_{t},  \mathbf{R}_{t})
      \end{split}
\label{EQ:KF}
\end{equation}
where $\mathcal{L}_{t}$ is the marginal likelihood describing the probability of observing observations $\mathbf{y}_{t}$ at time $t$ given all the observations up to time $t-1$ \cite{sarkka2013bayesian}. 
Note that the UD and Kalman filter are two different methods for calculating the same results. On one hand, the Kalman filter is faster and computationally simpler to implement and on the other hand, the UD filter is more robust toward numerical instabilities.   


The standard Kalman filter expressed in Eq.~\ref{EQ:KF} can process stationary, trend stationary, and acceleration stationary time series, but it is not capable of handling non-stationary time-series, which is needed when it comes to anomaly detection.
The generalization of the Kalman Filter for non-stationary time-series is found in the Switching Kalman filter (SKF) equations.

\subsection{Switching Kalman filter}
\label{SS:THSKF}
We may be interested in anomaly detection, that is, modelling and detecting the changes of behavior due to changes in the dynamics of the baseline response of the time-series.
One way to model changing dynamics is to run in parallel a collection of ${\mathtt{S}}$ linear models, each having their own system dynamics $\mathbf{A}_{t}$ and $ \mathbf{Q}_{t}$.
%In the Switching Kalman filter (SKF) approach \citep{Murphy1998}, a collection of $S$ linear models are run in parallel.
%Each linear model has its own system dynamics (i.e their own $\mathbf{A}_{t}$ and $ \mathbf{Q}_{t}$ matrices).
In such approach, a discrete markovian switching variable $s_{t}= 1, ..,j,.. ,\mathtt{S}$ with a transition probabilities matrix $\mathbf{Z}_{t}$ and probabilities $\bm{\pi}_{t}$ is introduced to indicate which dynamics is used at time $t$.
The problem of incorporating switching dynamics into the model is that the state vector grows in a way that the dimension of the state vector at time $t$ is $\mathtt{S}^{t}$.
Therefore, the estimation quickly becomes intractable.
One solution is to merge at each time $t$ the states sharing the same dynamics using gaussian mixture.
This technique, known as the Switching Kalman filter, allows to keep the dimension of the state vector equal to $S$ at each time $t$ \cite{murphy2012machine}.
The SKF algorithm can be divided into two successive steps, (i) the ``Filter'' and, (ii) the ``Collapse'' step.
Following the notation used in Eq.~\ref{EQ:KF} the first step can be expressed in its short form as:
\begin{equation}
  \begin{split}
  (\bm{\mu}_{t|t}^{i(j)},\bm{\Sigma}_{t|t}^{i(j)}, \mathcal{L}_{t}^{i(j)}) = \text{Filter}(\bm{\mu}_{t-1|t-1}^{i},\bm{\Sigma}_{t-1|t-1}^{i}, \mathbf{y}_{t}, \mathbf{A}_{t}^{j},  \mathbf{Q}_{t}^{i(j)},   \mathbf{C}^{j}_{t},  \mathbf{R}^{j}_{t})
    \end{split}
\label{EQ:SKF1}
\end{equation}
where the superscripts $i(j)$ indicates that the current state at time $t$ is $s_{t}=j$ given the state at time $t-1$ is $s_{t-1}=i$, and
$ \mathcal{L}_{t}^{i(j)}$  the marginal likelihood that describes the probability of observing observations $\mathbf{y}_{t}$ at time $t$ given all the observations up to time $t-1$, and given the state at time $t_1$ was $s_{t-1} = i$ and that it switches to $s_{t} = j$ at time $t$.
The state probability $\mathbf{\pi}_{t|t}^{j}$ at each time $t$ is computed from the previous state probabilities $\bm{\pi}_{t-1|t-1}$, the likelihood $\mathcal{L}_{t}^{i(j)}$, and the transition probability $Z_{t}^{i(j)}$, such as
\begin{equation}
\pi_{t|t}^{j} = \sum_{i=1}^{\mathtt{S}} \frac{\mathcal{L}_{t}^{i(j)} \pi_{t-1|t-1}^{i} Z^{i(j)}_{t} }{c},
\label{EQ:StateProbability}
\end{equation}
where $c$ is a normalization constant ensuring that $ \sum_{j=1}^{\mathtt{S}} \pi_{t|t}^{j} = 1 $.
Moreover, the state switching probability is defined as
\begin{equation}
W_{t-1|t}^{i(j)} = \frac{\mathcal{L}_{t}^{i(j)} \pi_{t-1|t-1}^{i} Z^{i(j)}_{t} }{c\pi_{t|t}^{j}}.
\label{EQ:StateSwitchingProbability}
\end{equation}
$W_{t|t-1}^{i(j)}$ are required to perform the ``Collapse'' step, which can be expressed in its short form as:
\begin{equation}
  \begin{split}
  (\bm{\mu}_{t|t}^{j},\bm{\Sigma}_{t|t}^{j}) = \text{Collapse}(\bm{\mu}_{t|t}^{i(j)},\bm{\Sigma}_{t|t}^{i(j)}, W_{t-1|t}^{i(j)} );
    \end{split}
\label{EQ:SKF2}
\end{equation}
where state switching probabilities $W_{t|t-1}^{i(j)}$ are used as weighting factors for the gaussian mixture.
From Eq.~\ref{EQ:SKF2}, the SKF algorithm provides a set a $\mathtt{S}$ state vectors at each time $t$.
However, for the ease of interpretation, it is generally more convenient to have a single state vector at each time $t$.
Therefore, we hereafter introduce the ``Merge'' step.
Similarly to the ``Collapse'' step of the SKF algorithm, the ``Merge'' step uses the gaussian mixture technique, and it can be expressed in its short form as:
\begin{equation}
  \begin{split}
  (\bm{\mu}_{t|t},\bm{\Sigma}_{t|t}) = \text{Merge}(\bm{\mu}_{t|t}^{j},\bm{\Sigma}_{t|t}^{j},  \pi_{t|t}^{j} );
    \end{split}
\label{EQ:SKFCollapse}
\end{equation}
where the state probabilities $\pi_{t|t}^{j}$ is used as weighting factors for the gaussian mixture \cite{Nguyen2018}.

\subsection{Model parameter estimation}
\label{SS:THModelParameterEstimation}
The matrices $\mathbf{A}_{t}$,  $\mathbf{Q}_{t}$,   $\mathbf{C}_{t}$ and  $\mathbf{R}_{t}$ depend on a set of model parameters $\bm{\theta}$.
In most cases, $\bm{\theta}$ are unknown, and they can be learned from a training dataset $\mathbf{y}_{1:\mathtt{Tr}}$.
The procedure of learning the model parameters is hereafter referred to as model parameters estimation.
\subsubsection{Maximum log A Posteriori (MAP)}

The log a posteriori probability density function (PDF) is defined as
\begin{equation}
%\begin{array}{rcl}
\ln p(\bm\theta|\mathbf{y}_{1:\mathtt{Tr}})  \propto \, \ln p(\mathbf{y}_{1:\mathtt{Tr}}|\bm\theta) + \ln p(\bm\theta),
%\end{array}
\label{EQ:BT}
\end{equation} 
where $p(\mathbf{y}_{1:\mathtt{Tr}}|\bm\theta)$ is the likelihood,  $p(\bm\theta)$ is the prior PDF.
The likelihood PDF is the joint prior probability of observations, that is, plausibility of the available observations $\mathbf{y}_{1:\mathtt{Tr}}$ given the parameter vector $\bm\theta$.  
Assuming that the observations are independent from each other, the joint log-likelihood function is defined as the sum of the marginal log-likelihoods, such as 
\begin{equation}
\ln p(\mathbf{y}_{1:\mathtt{Tr}}|\bm\theta)  = \displaystyle\sum_{t=1}^{\mathtt{Tr}} \ln p(\mathbf{y}_{t}|\mathbf{y}_{1:t-1},\bm \theta) = \displaystyle\sum_{t=1}^{\mathtt{Tr}} \ln \left[ \sum_{j=1}^{\mathtt{S}} \sum_{i=1}^{\mathtt{S}} \mathcal{L}_{t}^{i(j)} \pi_{t-1|t-1}^{i} Z_{t}^{i(j)} \right] \text{,}
\label{EQ:LP}
\end{equation}
where $\mathcal{L}_{t}^{i(j)}$ and  $\pi_{t-1|t-1}^{i}$ are computed at each time $t$ from the Switching Kalman Filter; $\mathtt{S}$ is the total number of model class, and the values of $Z_{t}^{i(j)}$ are known from the current set of model parameters.
The maximum log a posteriori procedure consists in identifying the point estimates by maximizing the log A Posteriori PDF, such as
\begin{equation*}
\bm\theta^{*} = \underset{\bm\theta}{\text{arg}\max}\left[\ln p(\bm\theta|\mathbf{y}_{1:\mathtt{Tr}}) \right] \text{,}
\end{equation*}
where $\bm\theta^{*}$ are the optimized model parameters values.

\subsubsection{Maximum log Likelihood Estimation (MLE)}

The Maximum log Likelihood Estimation (MLE) is a special case of the MAP where the prior PDF $p(\bm\theta)$ is assumed to be uniform \cite{gelman2014bayesian}.
Therefore, the Maximum log Likelihood procedure consists in identifying the point estimates by maximizing the log likelihood PDF, such as
\begin{equation*}
\bm\theta^{*} = \underset{\bm\theta}{\text{arg}\max}\left[\ln  p(\mathbf{y}_{1:\mathtt{Tr}}|\bm\theta) \right] \text{,}
\end{equation*}
where $\bm\theta^{*}$ are the optimized model parameters values.


\subsubsection{Laplace Approximation}

The MAP and MLE are point estimation methods which do not take into account the uncertainty in the parameter estimates $\bm\theta^{*}$. 
The estimation of the uncertainties in the model parameters estimates can be addressed using the Laplace approximation \cite{gelman2014bayesian} so that
$$p(\bm\theta|\mathbf{y}_{1:\mathtt{Tr}})  \approx  \mathcal{N}\left(\bm\theta;\bm\theta^{*},-\mathbf{H}(\bm\theta^{*})^{-1}\right)
\label{EQ: LaA}
$$
where $\mathbf{H}(\bm\theta^{*})$ is the second derivative of the log a posteriori or log likelihood PDF evaluated at the optimal parameter values $\bm\theta^{*}$. 

\subsubsection{Gradient-based optimization}

The gradient-based optimizations techniques are iterative approaches which can be used to find the model parameters that correspond to the maximum of a target PDF, hereafter noted $\mathcal{T}(\bm{\theta})$.
The function $\mathcal{T}(\bm{\theta})$  is either the log a posteriori or the log likelihood PDF computed from the data.
One iteration of gradient based algorithm is

\begin{equation}
{\bm\theta}_{\text{new}}  = {\bm\theta}_{\text{old}} - \eta \nabla \mathcal{T}(\bm{\theta}_{\text{old}}),
\label{EQ:GBA}
\end{equation}

where $\eta$ is the learning rate, and $\nabla$ the first derivative.

\paragraph{Parameter-wise Newton-Raphson}

The parameter-wise Newton-Raphson \cite{gelman2014bayesian} algorithm is an iterative approach which can be used to find the model parameters that correspond to the maximum of a target PDF, hereafter noted $\mathcal{T}_{1:\mathtt{Tr}}(\bm{\theta})$.
The underscripts $1:\mathtt{Tr}$ indicate that the target function is evaluated using a \emph{training dataset} of length $\mathtt{Tr}$.
%The function $\mathcal{T}_{1:\mathtt{Tr}}(\bm{\theta})$  is either the log a posteriori or the log likelihood PDF.
The Newton-Raphson algorithm adaptively sets the learning rate using the second derivative and a factor noted $\lambda$.
One \emph{iteration} of the Newton-Raphson algorithm is
\begin{equation}
{\theta}_{\text{new}}^{i}  = {\theta}_{\text{old}}^{i} - \lambda \frac{\nabla \mathcal{T}_{1:\mathtt{Tr}}(\bm{\theta}_{\text{old}}^{i}) }{  \nabla^{2} \mathcal{T}_{1:\mathtt{Tr}}(\bm{\theta}_{\text{old}}^{i})},
\label{EQ:NR}
\end{equation}
where $i$ is the index of the parameter being learned, $\nabla$ the first derivative, $\nabla^{2}$ the second derivative.
$\bm{\theta}_{\text{old}}$ and $\bm{\theta}_{\text{new}}$ are the previous and updated vector of model parameters. 
One parameter is updated at each iteration.
The convergence of each model parameters is reached when the following conditions are satisfied
\begin{equation}
\left\{\begin{array}{ccc}
\mathcal{T}_{1:\mathtt{Tr}}(\bm\theta^{i}_{\text{old}}) &<&\mathcal{T}_{1:\mathtt{Tr}}(\bm\theta^{i}_{\text{new}})\\[4pt]
\left|\mathcal{T}_{1:\mathtt{Tr}}(\bm\theta^{i}_{\text{new}}) -  \mathcal{T}_{1:\mathtt{Tr}}(\bm\theta^{i}_{\text{old}})\right| &\leq& \tau \cdot \left|\mathcal{T}_{1:\mathtt{Tr}}(\bm\theta^{i}_{\text{old}})\right|
\end{array}\right.,
\label{EQ:STC}
\end{equation}
where $\tau$ is a termination tolerance.
The Newton-Raphson algorithm stops when each model parameters has reached the convergence.

\paragraph{Stochastic gradient}

In the stochastic gradient technique, the target function and its derivatives are approximated at each iteration using a \emph{mini-batch} of data of length $\mathtt{Tb} \ll \mathtt{Tr}$.
Therefore, the target function is noted $\mathcal{T}_{1:\mathtt{Tb}}(\bm{\theta})$.
At each iteration, the beginning of the mini-batch is selected randomly.
One \emph{epoch} consists in one pass over the full training dataset (i.e. all the training data have been seen once).
Therefore, one epoch is made of many iterations.
Note that more than one model parameters is usually updated during one epoch.
Several epochs are needed to reach convergence.
%Classical implementation of stochastic gradient algorithm includes momentum approach (e.g MMT optimizer) or adaptive learning rate (e.g. Adam optimizer) to increase the performance \cite{Goodfellow-et-al-2016}.  
The convergence is reached when the following condition between two successive epochs is satisfied 
\begin{equation}
\mathcal{T}^{\text{epoch}} (\bm\theta) > \tau  \cdot \mathcal{T}^{\text{epoch-1}} (\bm\theta)
\label{EQ:SGT}
\end{equation}
where $0 \le \tau \le 1$ is a termination tolerance.
Classical implementation of stochastic gradient algorithm includes momentum approach (e.g MMT optimizer) or adaptive learning rate (e.g. Adam optimizer) to increase the performance \cite{Goodfellow-et-al-2016}.  

\paragraph{Approximation of the derivatives}

In most cases. the derivatives of $\mathcal{T}(\bm{\theta})$ can not be computed analytically.
Therefore, the derivatives are approximated numerically using the central differentiation scheme, such as
\begin{gather}
\begin{aligned}
 \nabla \mathcal{T}(\bm\theta^{i}) & = \frac{\partial \mathcal{T} (\bm\theta) }{\partial \theta^{i}} \approx \frac{\mathcal{T} (\bm\theta + \Pi(i)\Delta \theta^{i} )  -  \mathcal{T} (\bm\theta - \Pi(i)\Delta \theta^{i} ) }{2\Delta \theta^{i}}  \\
 \nabla^{2} \mathcal{T}(\bm\theta^{i}) & = \frac{\partial^{2} \mathcal{T} (\bm\theta) }{\partial^{2} \theta^{i}} \approx \frac{\mathcal{T} (\bm\theta + \Pi(i)\Delta \theta^{i} )  -  2 \mathcal{T} (\bm\theta) +  \mathcal{T} (\bm\theta - \Pi(i)\Delta \theta^{i} ) }{(\Delta \theta^{i})^{2}},
\label{EQ:numericaldiff}
\end{aligned}
\end{gather}
where $\Delta \theta^{i}$ is a small perturbation to the value of the $i^{\text{th}}$ model parameters and $\Pi(i)$ is an indicator vector for which all values are equal to $0$, except the $i^{\text{th}}$ value which is equal to one.


\subsubsection{Model parameter space transformation}
\label{SS:THSpaceTransformation}

There are some model parameters which are defined in a bounded interval.
For instance, the standard deviation model parameters are real numbers that lie in the $[0, +\infty]$ interval.
%The autoregression coefficient model parameters are real numbers that lie in the $[0, 1]$ interval.
Therefore, during the learning procedure, it may happen that new model parameters $\bm\theta_{\text{new}}$ are proposed outside their valid interval.
Those parameters must be rejected, which strongly hinders the computational efficiency of the learning algorithm.
The solution employed in OpenBDLM is to transform the bounded parameters space into an unbounded parameters space, where the parameters lie in the $[ -\infty, +\infty ]$ interval, and to perform the learning procedure in the transformed, unbounded, space.
The transformation is done using a function $g(.)$ so that, 
\begin{equation}
\theta^{\text{tr}} = g(\theta) \text{, }\quad \theta^{\text{tr}} \in [-\infty, +\infty ] \text{.}
\end{equation}
The choice of the function $g(.)$ depends on the bound of $\theta$. 
Three cases generally occur:
\begin{itemize}
\item $ \theta \in [-\infty, +\infty ]$ , $g(\theta) = 1$, so that $\theta^{\text{tr}} = \theta$ and $\theta = \theta^{\text{tr}} $
\item $ \theta \in [0, +\infty ]$, $g(\theta) = \ln(\theta)$, so that $\theta^{\text{tr}} = \ln(\theta) $ and $\theta = e^{\theta^{\text{tr}}} $
\item $ \theta \in [\text{a}, \text{b}]$, $g(\theta) = \text{sigmoid}(\theta)$, so that $\theta^{\text{tr}} = -\ln \left( \frac{b-a}{\theta-a} - 1\right) $, and $\theta = \left( \frac{b-a}{1+e^{-\theta^{\text{tr}}}} + a \right)$
\end{itemize}
For instance, the standard deviation model parameters are real numbers that lie in the $[0, +\infty]$ interval and the logarithm transformation is used.
Moreover, the autoregression coefficient model parameters are real numbers that lie in the $[0, 1]$ interval, and the sigmoid transformation is used.

\subsection{Block components}
\label{SS:BlockComponent}
The block components are pieces of the full model.
Each block component is used to describe a given dynamics for a given time-series.
Therefore, each block component has its own transition and observation model, which are associated with some model parameters.
Each block component can be associated with one or more hidden states variables.
%The types of block components supported in the current OpenBDLM version are listed in the next sections.
The block components are then assembled to build the full model.
The block components associated with irreversible change in the time series belongs to the \emph{baseline} component.
The other block components are associated with reversible change in the time-series.
The \emph{compatible} block component are needed to model switching dynamics in the baseline of the time-series.

\subsubsection{Local level (baseline)}

The local level block component describes the local mean of a stationary time-series (no trend and no acceleration) \cite{STC:STC2035}. 
The local level describes irreversible changes.\\

\noindent
Number of hidden states: 1\\

Hidden states vector: 
\begin{gather*}
\mathbf{x}^{\mathtt{LL}} = [x^{\mathtt{LL}}]
\end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{LL}}=[1]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{LL}}=[1]
\end{gather*}
Process noise covariance matrix: 
\begin{gather*}
\mathbf{Q}^{\mathtt{LL}}=[(\sigma_{w}^{\mathtt{LL}})^{2}]
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{LL}}=[\sigma_{w}^{\mathtt{LL}} ]
\end{gather*}

\noindent
$\sigma_{w}^{\mathtt{LL}}$ is the process noise standard deviation which can be learned from the data.

\subsubsection{Local trend (baseline)}

The local trend block component describes the local mean of a trend-stationary time-series (trend and no acceleration) \cite{STC:STC2035}. 
The local level describes irreversible changes.\\

\noindent
Number of hidden states: 2\\

Hidden states vector: 
\begin{gather*}
 \mathbf{x}^{\mathtt{LT}} = [x^{\mathtt{LL}}, x^{\mathtt{LT}}]^{\intercal}
 \end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{LT}}= \left[\begin{array}{cc}1 &\Delta t\\0&1\end{array}\right]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{LT}}=[1, 0]
\end{gather*}
Process noise covariance matrix: 
\begin{gather*}
\mathbf{Q}^{\mathtt{LT}}= (\sigma_{w}^{\mathtt{LT}})^{2}\left[\begin{array}{cc}\tfrac{\Delta t^{3}}{3} &\tfrac{\Delta t^{2}}{2}\\\tfrac{\Delta t^{2}}{2}&\Delta t\end{array}\right]
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{LT}}=[\sigma_{w}^{\mathtt{LT}} ]
\end{gather*}

\noindent
$\sigma_{w}^{\mathtt{LT}}$ is the process noise standard deviation, which can be learned from the data, and $\Delta t$ is the local timestep computed from the data.


\subsubsection{Local acceleration (baseline)}

The local acceleration block component describes the local mean of a acceleration-stationary time-series \cite{STC:STC2035}. 
It describes irreversible changes.\\

\noindent
Number of hidden states: 3\\

Hidden states vector: 
\begin{gather*}
\mathbf{x}^{\mathtt{LA}} = [x^{\mathtt{LL}}, x^{\mathtt{LT}} ,  x^{\mathtt{LA}}]^{\intercal}
\end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{LA}}=  \left[\begin{array}{ccc}1 &\Delta t&\Delta t^{2}\\0&1&\Delta t\\0&0&1\end{array}\right]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{LA}}=[1, 0, 0]
\end{gather*}
Process noise covariance matrix: 
\begin{gather*}
\mathbf{Q}^{\mathtt{LA}}=(\sigma_{w}^{\mathtt{LA}})^{2}\left[\begin{array}{ccc}\tfrac{\Delta t^{5}}{20} &\tfrac{\Delta t^{4}}{8} &\tfrac{\Delta t^{3}}{6}\\\tfrac{\Delta t^{4}}{8} &\tfrac{\Delta t^{3}}{3}&\tfrac{\Delta t^{2}}{2}\\\tfrac{\Delta t^{3}}{6}&\tfrac{\Delta t^{2}}{2}&\Delta t\end{array}\right]
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{LA}}=[\sigma_{w}^{\mathtt{LA}} ]
\end{gather*}

\noindent
$\sigma_{w}^{\mathtt{LA}}$ is the process noise standard deviation, which can be learned from the data, and $\Delta t$ is the local timestep computed from the data.



\subsubsection{Local level compatible trend (baseline)}

The local level trend compatible component must be used in case of model switching between a local level model and a local trend model \cite{Nguyen2018}.
The local level trend compatible block component describes the local mean of a stationary time-series. 
It describes irreversible changes.\\

\noindent
Number of hidden states: 1\\

Hidden states vector: 
\begin{gather*}
 \mathbf{x}^{\mathtt{LcT}} = [x^{\mathtt{LL}}, x^{\mathtt{LTc}}=0]^{\intercal}
 \end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{LcT}}= \left[\begin{array}{cc}1 & 0\\0&0\end{array}\right]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{LcT}}=[1, 0]
\end{gather*}
Process noise covariance matrix: 
\begin{gather*}
\mathbf{Q}^{\mathtt{LcT}}=(\sigma_{w}^{\mathtt{LcT}})^{2}\left[\begin{array}{cc}1 &0\\0&0\end{array}\right]
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{LcT}}=[\sigma_{w}^{\mathtt{LcT}} ]
\end{gather*}

\noindent
$\sigma_{w}^{\mathtt{LcT}}$ is the process noise standard deviation, which can be learned from the data, and $\Delta t$ is the local timestep computed from the data.

\subsubsection{Local level compatible acceleration (baseline)}

The local level acceleration compatible component must be used in case of model switching between a local level model and a local acceleration model \cite{Nguyen2018}.
The local level acceleration compatible block component describes the local mean of a stationary time-series.
It describes irreversible changes.\\

\noindent
Number of hidden states: 1\\

Hidden states vector:
\begin{gather*}
 \mathbf{x}^{\mathtt{LcA}} = [x^{\mathtt{LL}}, x^{\mathtt{LTc}}=0, x^{\mathtt{LAc}}=0]^{\intercal}
 \end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{LcA}}= \left[\begin{array}{ccc}1&0&0\\0&0&0\\0&0&0\end{array}\right]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{LcA}}=[1, 0, 0]
\end{gather*}
Process noise covariance matrix: 
\begin{gather*}
\mathbf{Q}^{\mathtt{LcA}}=(\sigma_{w}^{\mathtt{LcA}})^{2}\left[\begin{array}{ccc}1&0&0\\0&0&0\\0&0&0\end{array}\right]
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{LcA}}=[\sigma_{w}^{\mathtt{LcA}} ]
\end{gather*}

\noindent
$\sigma_{w}^{\mathtt{LcA}}$ is the process noise standard deviation, which can be learned from the data, and $\Delta t$ is the local timestep computed from the data.

\subsubsection{Local trend compatible acceleration (baseline)}
The local trend acceleration compatible component must be used in case of model switching between a local trend model and a local acceleration model \cite{Nguyen2018}.
The local trend acceleration compatible block component describes the local mean of a trend-stationary time-series. 
It describes irreversible changes.\\

\noindent
Number of hidden states: 2\\
Hidden states vector: 
\begin{gather*}
 \mathbf{x}^{\mathtt{TcA}} = [x^{\mathtt{LL}}, x^{\mathtt{LT}} , x^{\mathtt{LAc}}=0]^{\intercal}
 \end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{TcA}}= \left[\begin{array}{ccc}1&\Delta t&0\\0&1&0\\0&0&0\end{array}\right]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{TcA}}=[1, 0, 0]
\end{gather*}
Process noise covariance matrix: 
\begin{gather*}
\mathbf{Q}^{\mathtt{TcA}}=(\sigma_{w}^{\mathtt{TcA}})^{2}  \left[\begin{array}{ccc}\tfrac{\Delta t^{3}}{3} &\tfrac{\Delta t^{2}}{2}&0\\\tfrac{\Delta t^{2}}{2}&\Delta t&0\\0&0&0\end{array}\right] 
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{TcA}}=[\sigma_{w}^{\mathtt{TcA}} ]
\end{gather*}

\noindent
$\sigma_{w}^{\mathtt{TcA}}$ is the process noise standard deviation, which can be learned from the data, and $\Delta t$ is the local timestep computed from the data.



\subsubsection{Periodic (Fourier form)}

The periodic (Fourier form) block component describes a periodic pattern in the time-series using Fourier form \cite{west1999bayesian,STC:STC2035}. 
The periodic Fourier form allows modelling sine-like periodic pattern in time-series.
It describes reversible changes.\\

\noindent
Number of hidden states: 2\\

Hidden states vector: 
\begin{gather*}
\mathbf{x}^{\mathtt{P}} = [x^{\mathtt{P}_ {1}}, x^{\mathtt{P}_{2}}]^{\intercal}
\end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{P}}= \left[\begin{array}{cc}\cos \omega &\sin \omega\\-\sin \omega&\cos \omega\end{array}\right]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{P}}=[1, 0]
\end{gather*}
Process noise covariance matrix:
\begin{gather*}
\mathbf{Q}^{\mathtt{P}}=(\sigma_{w}^{\mathtt{P}})^{2}\left[\begin{array}{cc}1 &0\\0&1\end{array}\right]
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{P}}=[\sigma_{w}^{\mathtt{P}}, p^{\mathtt{P}} ]
\end{gather*}

\noindent
$\sigma_{w}^{\mathtt{P}}$ is the process noise standard deviation and $p$ the period in days, which can be learned from the data, and $\Delta t$ is the local timestep computed from the data.
 $\omega=\frac{2\pi \Delta t}{p}$ is the angular of frequency defined from the period $p$, given in days.


\subsubsection{Periodic (Kernel regression form)}

The periodic (Kernel regression form) block component describes a periodic pattern in the time-series using periodic kernel regression  \cite{Nguyen2019KRBDLM}. 
The periodic Kernel regression form allows modelling form-free periodic pattern in time-series.
It describes reversible changes.
The periodic kernel measures the similarity between pairs of covariates, and it is defined as
\begin{gather*}
k(t_{i},t_{j})=\exp\left[-\frac{2}{\ell^2}\sin\left( \pi\frac{t_i-t_{j}}{p}\right)^{2}\right].
\end{gather*}
The kernel output $k(t_{i},t_{j})\in(0,1)$ measures the similarity between two timestamps $t_{i}$ and $t_{j}$ as a function of the distance between these, as well as a function of two parameters; the period and kernel length, $\bm{\theta}=\{p,\ell\}$.
\noindent\\
Number of hidden states:  $\mathtt{L}^{\mathtt{KR}}+1$\\

Hidden states vector: 
\begin{gather*}
\mathbf{x}^{\mathtt{KR}} = [x^{\mathtt{KR}}_{0}, x^{\mathtt{KR}}_{1}, \dots, x^{\mathtt{KR}}_{\mathtt{L}^{\mathtt{KR}}}]^{\intercal}
\end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{KR}}= \left[\begin{array}{cc}0 &\tilde{\bm k}^{\mathtt{KR}}(t, \mathbf{t}^{\mathtt{KR}})\\\mathbf{0}&\mathbf{I}_{\mathtt{L}^{\mathtt{KR}}}\end{array}\right]
\end{gather*}
Process noise covariance matrix:
\begin{gather*}
\mathbf{Q}^{\mathtt{KR}}=\left[\begin{array}{cc}(\sigma_{w,0}^{\mathtt{KR}})^{2} &\mathbf{0}\\\mathbf{0}&(\sigma_{w,1}^{\mathtt{KR}})^{2}\cdot\mathbf{I}_{\mathtt{L}^{\mathtt{KR}} }\end{array}\right]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{KR}}=[1, 0, \dots, 0]
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{KR}}=[ p^{\mathtt{KR}},\ell^{\mathtt{KR}},  \sigma_{w,0}^{\mathtt{KR}}, \sigma_{w,1}^{\mathtt{KR}}]
\end{gather*}

\noindent
In the transition matrix, $\tilde{\bm k}^{\mathtt{KR}}(t,\mathbf{t}^{\mathtt{KR}})$ corresponds to the normalized kernel, $k(t,\mathbf{t}^{\mathtt{KR}})/\sum_{t} k(t,\mathbf{t}^{\mathtt{KR}})$. $\tilde{\bm k}^{\mathtt{KR}}(t,\mathbf{t}^{\mathtt{KR}})$ is parameterized by the kernel width $\ell^{\mathtt{KR}}$, its period $p^{\mathtt{KR}}$, and a vector of $\mathtt{L}^{\mathtt{KR}}$ timestamps $\mathbf{t}^{\mathtt{KR}}=[t_{1}^{\mathtt{KR}},\cdots,t_{\mathtt{L}^{\mathtt{KR}}}^{\mathtt{KR}}]$ where each timestamp $t_{i}^{\mathtt{KR}}$ is associated with a hidden control point value $x_{i}^{\mathtt{KR}}$. 
$\sigma_{w,1}^{\mathtt{KR}}$ controls the increase in the  variance of the hidden control points between successive time steps and $\sigma_{w,0}^{\mathtt{KR}}$ controls the time-independent process noise in the hidden predicted pattern.
$p^{\mathtt{KR}}$ and $\ell^{\mathtt{KR}}$ give the period and correlation length of the kernel, respectively.


\subsubsection{First order autoregressive}

The first order autoregressive component describes the time-dependent model errors (i.e the residual between the model prediction and the data) \cite{STC:STC2035}. 
It describes reversible changes.\\

\noindent
Number of hidden states: 1\\

Hidden states vector: 
\begin{gather*}
\mathbf{x}^{\mathtt{AR}} = [x^{\mathtt{AR}}]
\end{gather*}
Transition matrix: 
\begin{gather*}
\mathbf{A}^{\mathtt{AR}}=  [\phi^{\mathtt{AR}}]
\end{gather*}
Observation matrix: 
\begin{gather*}
\mathbf{C}^{\mathtt{AR}}=[1]
\end{gather*}
Process noise covariance matrix: 
\begin{gather*}
\mathbf{Q}^{\mathtt{AR}}=[(\sigma_{w}^{\mathtt{AR}})]
\end{gather*}
Model parameters: 
\begin{gather*}
\bm\theta^{\mathtt{AR}}=[\sigma_{w}^{\mathtt{AR}}, \phi^{\mathtt{AR}} ]
\end{gather*}

\noindent
$\sigma_{w}^{\mathtt{AR}}$ is the process noise standard deviation, and $\phi^{\mathtt{AR}}$ the autoregressive coefficient.

\subsection{Handling non-uniform time vector and missing data}
Label{SS:HandlingNonUniformMissingData}

\subsubsection{Non-uniform time vector}
\label{SS:NonUniform}

Non uniform time vector occurs when the time between two successive data measurements (i.e. the timestep) varies with time.
In order to accommodate non-uniform time vector, a reference time step $\Delta t^{\text{ref}} $ must be defined \cite{STC:STC2035}. 
The reference time-step is a value corresponding to the most frequent time step in the time series.
All parameter values in the parameter set $\bm \theta$ are estimated for the reference time step. 
Therefore, for local time step $\Delta t$ different than the reference timestep $\Delta t^{\text{ref}} $, the parameters value must be adapted accordingly.
It is proposed to scale the model error standard deviations $\sigma_{w}$ in $\mathbf{Q}_{t}$  proportionally to the ratio between the current time step and the reference time step so that,
\begin{gather*}
\sigma_{w}^{\Delta t}= \sigma_{w}^{\Delta t ^{\text{ref}}}\frac{\Delta t}{\Delta t ^{\text{ref}}}.
\end{gather*}
Therefore, the amount of process noise in the prediction model increases as the local time step increase with respect to the reference time step.

The transition matrix $\mathbf{A}^{\mathtt{AR}}$ contains the autoregressive coefficients $\phi^{\mathtt{AR}}$ that are recursively multiplied with the hidden state at each time step. 
To account for time step changes, the autoregressive coefficients are elevated to the power of the ratio between the current time step and the reference time step, such as
\begin{gather*}
\phi^{\mathtt{AR}, \Delta t}=  (\phi^{\mathtt{AR}, \Delta t ^{\text{ref}}})^{\frac{\Delta t}{\Delta t ^{\text{ref}}}}.
\end{gather*}
Therefore, the autocorrelation between successive data samples in the autoregressive prediction model decreases as the local time step increase with respect to the reference time step.

\subsubsection{Missing data (NaN)}

The presence of missing data (NaN) for specific time impedes to perform the full Kalman computations as the update step can not be performed \cite{STC:STC2035}.
However, the prediction step using the current transition model can be done.
%No update is performed at times associated with missing data (NaN), and only the prediction step is performed.
Therefore, BDLM can fill the gap when data are missing using the transition model.


\subsection{Dependencies between time-series}
\label{S:Dependencies}
The dependencies between time-series are handled by adding regression coefficients $\phi^{i|j}$ in the observation matrix.
For a dataset with $\mathtt{D}$ time-series, the observation matrix is
\begin{equation*}
\mathbf{C}=\left[\begin{array}{cccccc}
\mathbf{C}^{1}& \mathbf{C}_{1,2}^{c}&\cdots & \mathbf{C}_{1,j}^{c}&\cdots& \mathbf{C}_{1,\mathtt{D}}^{c}\\
\mathbf{C}_{2,1}^{c}& \mathbf{C}^{2}&\cdots& \mathbf{C}_{2,j}^{c}&\cdots& \mathbf{C}_{2,\mathtt{D}}^{c}\\
\vdots&\vdots& \vdots& \vdots& \ddots& \vdots\\
\mathbf{C}_{i,1}^{c}& \mathbf{C}_{i,2}^{c}&\cdots&\mathbf{C}_{i,j}^{c}&\cdots&\mathbf{C}_{i,\mathtt{D}}^{c}\\
\vdots&\vdots& \vdots& \vdots& \ddots& \vdots\\
\mathbf{C}_{\mathtt{D},1}^{c}& \mathbf{C}_{\mathtt{D},2}^{c}&\cdots& \mathbf{C}_{\mathtt{D},j}^{c}&\cdots& \mathbf{C}^{\mathtt{D}}
\end{array}\right] \text{.}
\end{equation*}
The dependence matrix is a matrix with $0$ and $1$ which is used to indicate which time series have dependencies between each others, such as 

\begin{equation*}
\mathbf{D}=\left[\begin{array}{cccccc}
1&d_{1,2}&\cdots&d_{1,j}&\cdots&d_{1,\mathtt{D}}\\
d_{2,1}&1&\cdots&d_{2,j}&\cdots&d_{2,\mathtt{D}}\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\
d_{i,1}&d_{i,2}&\cdots&1&\cdots&d_{i,\mathtt{D}}\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\
d_{\mathtt{D},1}&d_{\mathtt{D},2}&\cdots&d_{\mathtt{D},j}&\cdots&1\\
\end{array}\right] \text{.}
\end{equation*}
Then, 
\begin{itemize}
\item if $d_{i,j}=0$, $\mathbf{C}_{i,j}^{c}=[\mathbf{0}]$
\item if $d_{i,j}=1$, $\mathbf{C}_{i,j}^{c}=\left[\phi^{i|j}_{1},\phi^{i|j}_{2},\cdots,\phi^{i|j}_{k_{j}}\right]$ where $k_{j}$ is the number of hidden states associated with the $j^{th}$ time-series.
\end{itemize}
The regression coefficient $\phi^{i|j}_{k}$ gives the linear dependence between the $k^{th}$ hidden states of the $j^{th}$ time series and the $i^{th}$ time series.
In OpenBDLM, a dependence model between time-series assigns regression coefficient for the observed hidden states associated with block component describing reversible behavior (periodic and autoregressive patterns).
